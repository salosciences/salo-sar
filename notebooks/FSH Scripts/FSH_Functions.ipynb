{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Stand Height Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import math as mt\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import subprocess\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import argparse\n",
    "import pdb\n",
    "from osgeo import gdal, osr\n",
    "import string\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crop_ISCE:\n",
    "    xmlfile = \"resampleOnlyImage.amp.xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    size_array = np.array([])\n",
    "    for size in root.iter('property'):\n",
    "        if size.items()[0][1] == 'size':\n",
    "            size_array = np.append(size_array, int(size.find('value').text))\n",
    "    width = size_array[0]\n",
    "    length = size_array[1]\n",
    "    \n",
    "    nanval = 0\n",
    "    \n",
    "    # Read amp files in radar coordinates\n",
    "    amp_file = np.fromfile(\"resampOnlyImage.amp\", dtype = 'complex64')\n",
    "    inty = amp_file.reshape((length, width))\n",
    "    \n",
    "    # Creating empty array for cropped square list\n",
    "    \n",
    "    inty[:176,:] = nanval\n",
    "    inty[5488:,:] = nanval\n",
    "    inty[:,:163] = nanval\n",
    "    inty[:,4846:] = nanval\n",
    "    \n",
    "    # Write output files\n",
    "    inty.tofile(\"resampOnlyImage.amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crop_ROIPAC(directory, date1, date2):\n",
    "    # Extract ROI_PAC parameters\n",
    "    amp_rsc_file = date1 + \"-\" + date2 + \"-\" + \"_2rlk.amp.rsc\"\n",
    "    width = int(rrd.read_rsc_data(amp_rsc_file, directory, \"WIDTH\"))\n",
    "    length = int(rrd.read_rsc_data(amp_rsc_file, directory, \"FILE_LENGTH\"))\n",
    "    fullwidth = width*2\n",
    "    nanval = 0\n",
    "    \n",
    "    # Readcpr files in radar coordinates\n",
    "    cor_file = np.fromfile(directory + date1 + \"-\" + date2 + \"_2rlks.cor\", dtype = \"f4\", count = length*fullwidth)\n",
    "    corr = cor_file.reshape((length, fullwidth))\n",
    "    mag = corr[:, 0:width]\n",
    "    phs = corr[:.width:fullwidth]\n",
    "    \n",
    "    # Read amp files in radar coordinates\n",
    "    amp_file = np.fromfile(directory + date1 + \"-\" + date2 + \"_2rlks.amp\", dtype = 'complex64')\n",
    "    inty = amp_file.reshape((length, width))\n",
    "    \n",
    "    # Creating empty array for cropped square list\n",
    "    mag[:638, :] = nanval\n",
    "    mag[3288:, :] = nanval\n",
    "    mag[:,:84] = nanval\n",
    "    mag[:,2418:] = nanval\n",
    "    \n",
    "    phs[:638, :] = nanval\n",
    "    phs[3288:, :] = nanval\n",
    "    phs[:,:84] = nanval\n",
    "    phs[:,2418:] = nanval\n",
    "    \n",
    "    inty[:638, :] = nanval\n",
    "    inty[3288:, :] = nanval\n",
    "    inty[:,:84] = nanval\n",
    "    inty[:,2418:] = nanval\n",
    "    \n",
    "    # Writing values\n",
    "    c_out[:, 0:width] = mag\n",
    "    c_out[:,width:fullwidth] = phs\n",
    "    \n",
    "    # Write output files\n",
    "    cx = c_out.astype('f4')\n",
    "    cx.tofile(directory + date1 + \"-\" + date2 + \"_2rlks_fix.cor\")\n",
    "    inty.tofile(directory + date1 + \"-\" + date2 + \"_2rlks_fix.amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_sinc(x, C_param):\n",
    "    # Get rid of extreme values by set all values where x > 1 equal to 1, and x < 0 equal to 0\n",
    "    x[(x > 1)] = 1\n",
    "    x[(x < 0)] = 0\n",
    "    \n",
    "    # Create array of increments between 0 and pi of size pi/100\n",
    "    XX = linspace(0, math.pi, num = 100, endpoint = True)\n",
    "    \n",
    "    # Set the first value of XX to eps to avoid division by zero issues\n",
    "    XX[0] = spacing(1)\n",
    "    \n",
    "    # Calculate sinc for for XX and save it to YY\n",
    "    ## YY - sinc(XX / math.pi)\n",
    "    YY = np.sin(XX) / XX\n",
    "    \n",
    "    # Reset the first value of XX to zero and the first value of YY to the corresponding output\n",
    "    XX[0] = 0\n",
    "    YY[0] = 1\n",
    "    \n",
    "    # Set the last value of YY to 0 to avoid NaN issues\n",
    "    YY[-1] = 0\n",
    "    \n",
    "    # Flip XX and YY left to right\n",
    "    YY = YY[::-1]\n",
    "    XX = XX[::-1]\n",
    "    \n",
    "    # Run interpolation\n",
    "    # XX and YY are your original values, x is the query value, and y is the interpolated values that correspond to x\n",
    "    y = interp_func(x)\n",
    "    \n",
    "    # Set all values in y less than 0 equal to 0\n",
    "    y[(y < 0)] = 0\n",
    "    # return y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters are the numbers of scenes, edges, start scene, iterations, the input/output file directory, \n",
    "    # averaging numbers in lat and lon for \"self\" and \"pairwise\" fitting, bin_size for density calculation in scatter plot fitting, \n",
    "    # flag for sparse data cloud filtering.\n",
    "def auto_mosaicking_new(scenes, edges, start_scene, N, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse):\n",
    "    # Set average S and C parameters (0 < s < 1, 0 < c < 20 so s = 0.65, and c = 13)\n",
    "    avg_S = 0.65\n",
    "    avg_C = 13\n",
    "    \n",
    "    # Create avg_dp matric, and fill with average S and C parameters\n",
    "    avg_dp = zeros(scenes * 2)\n",
    "    put(avg_dp, range(0, scenes * 2, 2), avg_S)\n",
    "    put(avg_dp, range(1, scenes * 2, 2), avg_C)\n",
    "    \n",
    "    # Create the dp matrix\n",
    "    # the difference of the avg and the initial SC values OR all the zeros (avg)\n",
    "    dp = zeros(scenes * 2)\n",
    "    \n",
    "    # Intialize target matrix nd fill with K = 1, B = 0\n",
    "    target_KB = zeros((edges + 1) * 2)\n",
    "    put(target_KB, range(0, (edges + 1) * 2, 2), 1)\n",
    "    \n",
    "    # Run cal_KB()\n",
    "    Y = cKB.cal_KB(dp, edges, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "    \n",
    "    # Calculate the residual for cal_KB - target\n",
    "    res = sum((Y - target_KB)**2)\n",
    "    \n",
    "    # Save dp and the residuals as the first iteration output file (using JSON)\n",
    "    iter_file = open(os.path.join(dirextory, \"output\", \"SC_0_iter.json\"), 'w')\n",
    "    json.dump([dp.tolist(), res], iter_file)\n",
    "    iter_file.close()\n",
    "    \n",
    "    # For the rest of the iterations run ls_deltaSC() and save to output file (using JSON)\n",
    "    for i in range(1, N + 1, 2): # This will run from i = 1 to i = N\n",
    "        [dp, res] = lSC.ls_deltaSC(dp, edges, scenes, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "        print(\"%d iterations completed!\\n\" % i)\n",
    "        print(time.strftime(\"%H:%M:%S\"))\n",
    "        filename = \"SC_%d_iter.json\" % i\n",
    "        iter_file = open(os.path.join(directory, \"output\", filename), 'w')\n",
    "        json.dump([dp.tolist(), res], iter_file)\n",
    "        iter_file.close()\n",
    "        \n",
    "    print(\"auto_mosaicking_new finsihed at \" + (time.strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tree_height_many(scenes, flagfile, directory, numLooks, noiselevel, flag_proc, flag_grad):\n",
    "    # for each scene name the file, run auto_tree_height_single and save the output to a .json file\n",
    "    for i in range(scenes):\n",
    "        \n",
    "        # Get the scene data and set the file name and image folder name (f#_o# where # is the frame and orbit numbers, respectively)\n",
    "        scene_data = fsf.flag_scene_file(flagfile, i + 1, directory) # 0 vs1 indexing\n",
    "        filename = scene_data[1]\n",
    "        image_folder = \"f\" + scene_data[4] + \"_o\" + scene_data[5] + \"/\"\n",
    "        \n",
    "        # Run auto_tree_height_single\n",
    "        if flag_proc == 0:\n",
    "            ######## ROI_PAC results\n",
    "            file_date = athsR.auto_tree_height_single_ROIPAC(directory + image_folder, scene_data[2], scene_data[3], numLooks, noiselevel, flag_grad)\n",
    "        elif flag_proc == 1:\n",
    "            ######## ISCE results\n",
    "            file_data = athsI.auto_tree_height_single_ISCE(directory + image_folder, scene_data[2], scene_data[3], numLooks, noiselevel, flag_grad)\n",
    "        else:\n",
    "            print(\"Invalid processor provided\")\n",
    "            \n",
    "        linkfile = directory + image_folder + filename + '_orig.mat'\n",
    "        \n",
    "    sio.savemat(linkfile, {'corr_vs':file_data[0], 'ks':file_data[1], 'coords':file_data[2]})\n",
    "    \n",
    "    # Write geodata to a text file (4th - 9th values in file_data) \n",
    "    geofile = open(directory + image_folder + filename + \"_geo.txt\", \"w\")\n",
    "    geofile.write(\"width: %d \\n\" % file_data[3])\n",
    "    geofile.write(\"nlines: %d \\n\" % file_data[4])\n",
    "    geofile.write(\"corner_lat: %f\" % file_data[5])\n",
    "    geofile.write(\"corner_lon: %f\" % file_data[6])\n",
    "    geofile.write(\"post_lat: %f\" % file_data[7])\n",
    "    geofile.write(\"post_lon: %f\" % file_data[8])\n",
    "    geofile.close()\n",
    "    \n",
    "    print (\"auto_tree_height_many finished at \" + (time.strftime(\"H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tree_height_single_ISCE(directory, date1, date2, numLooks, noiselevel, flag_grad):\n",
    "    # Extract ISCE parameters\n",
    "    xmlfile = subprocess.getoutput('find ' + directory + 'int_' + date1 + '_' + date2 + '/ -name *Proc.xml')\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    root_tag = root.tag\n",
    "    \n",
    "    range_pixel_res = float(root.findall(\"./master/instrument/range_pixel_size\")[0].text)\n",
    "    llambda = float(root.findall(\"./master/instrument/radar_wavelength\")[0].text)\n",
    "    try:\n",
    "        first_range = float(root.findall(\"./runTopo/inputs/range_first_sample\")[0].text)\n",
    "    except:\n",
    "        first_range = float(root.findall(\"./runTopo/inputs/RANGE_FIRST_SAMPLE\")[0].text)\n",
    "    try:\n",
    "        num_range_bin = int(root.findall(\"./runTopo/inputs/width\")[0].text)\n",
    "    except:\n",
    "        num_range_bin = int(root.findall(\"./runTopo/inputs/WIDTH\")[0].text)\n",
    "    try:\n",
    "        num_range_looks = int(root.findall(\"./runTopo/inputs/number_range_looks\")[0].text)\n",
    "    except:\n",
    "        num_range_looks = int(root.findall(\"./runTopo/inputs/NUMBER_RANGE_LOOKS\")[0].text)\n",
    "    center_range = first_range + (num_range_bin/2-1)*range_pixel_res*num_range_looks\n",
    "    incid_angle = float(root.findall(\"./master/instrument/incidence_angle\")[0].text)\n",
    "    baseline_top = float(root.findall(\"./baseline/perp_baseline_top\")[0].text)\n",
    "    baseline_bottom = float(root.findall(\"./baseline/perp_baseline_bottom\")[0].text)\n",
    "    baseline = (baseline_bottom+baseline_top)/2\n",
    "    \n",
    "\n",
    "    xmlfile = directory+\"int_\"+date1+\"_\"+date2+\"/topophase.cor.geo.xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    delta_array = np.array([])\n",
    "    start_array = np.array([])\n",
    "    size_array = np.array([], dtype=np.int32)\n",
    "    for size in root.iter('property'):\n",
    "        if size.items()[0][1] == 'size':\n",
    "            size_array = np.append(size_array, int(size.find('value').text))\n",
    "    for delta_val in root.iter('property'):\n",
    "        if delta_val.items()[0][1] == 'delta':\n",
    "            delta_array = np.append(delta_array, float(delta_val.find('value').text))\n",
    "    for start_val in root.iter('property'):\n",
    "        if start_val.items()[0][1] == 'startingvalue':\n",
    "            start_array = np.append(start_array, float(start_val.find('value').text))\n",
    "    end_array = start_array + size_array * delta_array\n",
    "    north = max(start_array[1],end_array[1])\n",
    "    south = min(start_array[1],end_array[1])\n",
    "    east = max(start_array[0],end_array[0])\n",
    "    west = min(start_array[0],end_array[0])\n",
    "    coords = [north, south, west, east]\n",
    "    geo_width = size_array[0]\n",
    "    geo_nlines = size_array[1]\n",
    "    corner_lat = north\n",
    "    corner_lon = west\n",
    "    step_lat = delta_array[1]\n",
    "    step_lon = delta_array[0]\n",
    "\n",
    "    xmlfile = directory+\"int_\"+date1+\"_\"+date2+\"/resampOnlyImage.amp.geo.xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    delta_array = np.array([])\n",
    "    start_array = np.array([])\n",
    "    size_array = np.array([], dtype=np.int32)\n",
    "    for size in root.iter('property'):\n",
    "        if size.items()[0][1] == 'size':\n",
    "            size_array = np.append(size_array, int(size.find('value').text))\n",
    "    if (size_array[0]<geo_width)|(size_array[1]<geo_nlines):\n",
    "        for delta_val in root.iter('property'):\n",
    "            if delta_val.items()[0][1] == 'delta':\n",
    "                delta_array = np.append(delta_array, float(delta_val.find('value').text))\n",
    "        for start_val in root.iter('property'):\n",
    "            if start_val.items()[0][1] == 'startingvalue':\n",
    "                start_array = np.append(start_array, float(start_val.find('value').text))\n",
    "        end_array = start_array + size_array * delta_array\n",
    "        north = max(start_array[1],end_array[1])\n",
    "        south = min(start_array[1],end_array[1])\n",
    "        east = max(start_array[0],end_array[0])\n",
    "        west = min(start_array[0],end_array[0])\n",
    "        coords = [north, south, west, east]\n",
    "        geo_width = size_array[0]\n",
    "        geo_nlines = size_array[1]\n",
    "        corner_lat = north\n",
    "        corner_lon = west\n",
    "        step_lat = delta_array[1]\n",
    "        step_lon = delta_array[0]\n",
    "\n",
    "\n",
    "    # Read geolocated amp and cor files\n",
    "\n",
    "    fid_cor = open(directory + \"int_\"+date1+\"_\"+date2+\"/topophase.cor.geo\", \"rb\")\n",
    "    cor_file = np.fromfile(fid_cor, dtype=np.dtype('<f'))\n",
    "\n",
    "    corr = cor_file.reshape(2*geo_width, -1, order='F')\n",
    "    corr = corr[:,0:geo_nlines]\n",
    "    corr_mag = corr[geo_width:2*geo_width,:]\n",
    "\n",
    "    fid_amp = open(directory + \"int_\"+date1+\"_\"+date2+\"/resampOnlyImage.amp.geo\", \"rb\")\n",
    "    amp_file = np.fromfile(fid_amp, dtype=np.dtype('<f'))\n",
    "    inty = amp_file.reshape(2*geo_width, -1, order='F')\n",
    "    inty = inty[:,0:geo_nlines]\n",
    "    inty1 = inty[::2,:]\n",
    "    inty2 = inty[1::2,:]\n",
    "\n",
    "\n",
    "    # Operations\n",
    "    inty1 = np.power(inty1,2) # Hardcoded based on 2 range looks and 10 azimuth looks\n",
    "    inty2 = np.power(inty2,2)\n",
    "\n",
    "    inty1[inty1 <= 0] = np.NaN\n",
    "    inty2[inty2 <= 0] = np.NaN\n",
    "    corr_mag[corr_mag <= 0] = np.NaN\n",
    "\n",
    "    ####### Noise level for ISCE-processed SAR backscatter power output\n",
    "    if noiselevel == 0.0:\n",
    "        if root_tag[0] == 'i':\n",
    "            ####### ALOS thermal noise level (insarApp)\n",
    "            N1 = 55.5**2\n",
    "            N2 = 55.5**2\n",
    "        elif root_tag[0] == 's':\n",
    "            ####### ALOS thermal noise level (stripmapApp)\n",
    "            N1 = (55.5/81)**2\n",
    "            N2 = (55.5/81)**2\n",
    "        else:\n",
    "            raise Exception(\"invalid *Proc.xml file!!!\")\n",
    "    else:\n",
    "        N1 = noiselevel\n",
    "        N2 = noiselevel\n",
    "\n",
    "\n",
    "    S1 = inty1 - N1\n",
    "    g_th_1 = np.zeros(S1.shape)\n",
    "    g_th_1[S1>N1] = np.sqrt(S1[S1>N1] / (S1[S1>N1] + N1))\n",
    "    g_th_1[np.isnan(S1)] = np.NaN\n",
    "    g_th_1[S1 <= N1] = np.NaN\n",
    "\n",
    "    S2 = inty2-N2\n",
    "    g_th_2 = np.zeros(S2.shape)\n",
    "    g_th_2[S2>N2] = np.sqrt(S2[S2>N2] / (S2[S2>N2] + N2))\n",
    "    g_th_2[np.isnan(S2)] = np.NaN\n",
    "    g_th_2[S2 <= N2] = np.NaN\n",
    "\n",
    "    g_th = g_th_1 * g_th_2\n",
    "\n",
    "    corr_mag[corr_mag<0] = 0\n",
    "    corr_mag[corr_mag>1] = 1\n",
    "    corr_mag = rcb.remove_corr_bias(corr_mag,numLooks)\n",
    "    corr_mag[corr_mag<0] = 0\n",
    "\n",
    "    corr_vs = corr_mag / g_th\n",
    "\n",
    "    # set constants\n",
    "    pi=mt.pi\n",
    "\n",
    "    # correcting geometric decorrelation related to value compensation of ROI result compared to GAMMA. Caused by baseline/other decorrelation\n",
    "    gamma_base = 1 - (2 * mt.fabs(baseline) * mt.cos(incid_angle / 180 * pi) * range_pixel_res / mt.sin(incid_angle / 180 * pi) / llambda / center_range)\n",
    "    gamma_geo = gamma_base\n",
    "    corr_vs = corr_vs / gamma_geo\n",
    "    corr_vs[corr_vs>1] = 1\n",
    "\n",
    "    ##### Simple Radiometric correction of the coherences\n",
    "    if flag_grad == 1:\n",
    "        y = np.linspace(1, geo_width, geo_width)\n",
    "        x = np.linspace(1, geo_nlines, geo_nlines)\n",
    "        [X, Y] = np.meshgrid(x, y)\n",
    "        A = np.vstack([X[~np.isnan(corr_vs)], Y[~np.isnan(corr_vs)], np.ones(np.size(corr_vs[~np.isnan(corr_vs)]))]).T\n",
    "        coeff = np.linalg.lstsq(A, corr_vs[~np.isnan(corr_vs)])[0]\n",
    "        corr_vs = corr_vs - X*coeff[0] - Y*coeff[1]\n",
    "        corr_vs[corr_vs>1] = 1\n",
    "        corr_vs[corr_vs<0] = 0\n",
    "\n",
    "    kz = -2 * pi * 2 / llambda / center_range / mt.sin(incid_angle/180*pi) * baseline\n",
    "    kz = mt.fabs(kz)\n",
    "\n",
    "    # Return corr_vs, kz, coords\n",
    "    return corr_vs, kz, coords, geo_width, geo_nlines, corner_lat, corner_lon, step_lat, step_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rsc_data(filename, directory, param):\n",
    "    # set default output value\n",
    "    result = -1\n",
    "    \n",
    "    # Set filenmae for file to be searched\n",
    "    rsc_file = dirextory + filenmae\n",
    "    \n",
    "    # Read parameters from file\n",
    "    for line in open(rsc_file):\n",
    "        if line.startswith(param):\n",
    "            result = float(line.strip().split()[1])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_KB(dp, edges, start_scene, link, directory, Nd_pairwise, Nd_self, bin_size .flag_sparse):\n",
    "    \n",
    "    # make output matric of zeros \n",
    "    YY = zeros((edges + 1) * 2)\n",
    "    if link.size != 0:\n",
    "        # for each edge run cal_KB_pairwise_new an put theoutput into YY\n",
    "        fir i in range(edges):\n",
    "            k_temp, b_temp = kbp.cal_KB_pairwise_new(int(link[i, 0]), int(link[i, 1]), dp[int((2*link[i, 0])-2)], dp[int((2*link[i, 0])-1)], dp[int((2*link[i, 1])-2)], dp[int((2*link[i, 1])-1)], directory, Nd_pairwise, bin_size)\n",
    "            YY[2 * i] = k_temp\n",
    "            YY[(2 * i)] = b_temp\n",
    "            \n",
    "        # run cal_KB_self_new and put output into YY\n",
    "        \n",
    "        k_temp, b_temp = kbs.cal_KB_self_new(dp[int((2 * start_scene) - 2)], dp[int((2 * start_scene) - 1)], directory, Nd_self, bin_size, flag_sparse)\n",
    "        YY[(2 * (edges + 1)) - 2] = k_temp\n",
    "        YY[(2 * (edges + 1)) - 1] = b_temp\n",
    "        \n",
    "        # Return Y\n",
    "        return YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_KB_pairwise_new(scene1, scene2, deltaS1, deltaC1, deltaS2, deltaC2, directory, Nd_pairwise, bin_size):\n",
    "    # Set main file name string as scene1_scene2\n",
    "    file_str == str(scene1) + '_' + str(scene2)\n",
    "    \n",
    "    # Load and read data from .mat file\n",
    "    # Samples and lines are calculated from the shape of the images\n",
    "    selffile_data = sio.loadmat(directory + \"output/\" + file_str + \".mat\")\n",
    "    image1 = selffile_data['I1']\n",
    "    image2 = selffile_data['I2']\n",
    "    lines = int(image1.shape[0])\n",
    "    samples = int(image1.shape[1])\n",
    "    \n",
    "    # S and C parameters are the average S and C plus the delta value\n",
    "    S_param1 = 0.65 + deltaS1\n",
    "    S_param2 = 0.65 + deltaS2\n",
    "    C_param1 = 13 + deltaC1\n",
    "    C_param2 = 13 + deltaC2\n",
    "    \n",
    "    # Create gamma and ren arc_since for image1\n",
    "    gamm1 = image1.copy()\n",
    "    gamma1 = gamma1 / S_param1\n",
    "    image1 = arc.arc_since(gamma1, C_param1)\n",
    "    image1[isnan(gamma1)] = nan\n",
    "    \n",
    "    # Create gamma and run arc_since for image2\n",
    "    gamma2 = image2.copy()\n",
    "    gamma2 = gamma2 / S_param2\n",
    "    image2 = arc.arc_since(gamma2, C_param2)\n",
    "    image2[isnan(gamma2)] = nan\n",
    "    \n",
    "    # Partition image into subsections for noise suppression (multi-step process)\n",
    "    # Create M and N which are the number of subsections in each direction; fix() rounds towards zero\n",
    "    # NX and NY are the subsection dimensions\n",
    "    NX = Nd_pairwise\n",
    "    NY = Nd_pairwise\n",
    "    M = int(fix(lines / NY))\n",
    "    N = int(fix(samples / NX))\n",
    "    \n",
    "    # Create JM and JN which is the remainder adter dividing into subsections\n",
    "    JM = lines % NY\n",
    "    JN = samples % NX\n",
    "    \n",
    "    # Select the portion of images that are within the subsections\n",
    "    image1 = image1[0:lines - JM][:, 0:samples - JN]\n",
    "    image2 = image2[0:lines - JM][:, 0:samples - JN]\n",
    "    \n",
    "    # Split each image into subsections and run mean_wo_nan on each subsection\n",
    "    \n",
    "    # Declare new arrays to hold the subsection averages\n",
    "    image1_means = zeros((M, N))\n",
    "    image2_means = zeros((M, N))\n",
    "    \n",
    "    # Processing image1\n",
    "    # Split image into subsections with NY of rows in each\n",
    "    \n",
    "    image1_rows = split(image1, M, 0)\n",
    "    for i in range(M):\n",
    "        # Split each section into subsections with NX number of columns in each\n",
    "        row_array_split(image1_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean with NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image1_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "            \n",
    "    # Processing image2\n",
    "    # split image into subsections with NY number of rows in each\n",
    "    image2_rows = split(image2, M, 0)\n",
    "    for i in range(M):\n",
    "        # split each section into subsections with NX number of columns ineach\n",
    "        row_array = split(image2_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image2_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "            \n",
    "    # Make an array for each image of where mean > 0 for both images\n",
    "    IND1 = logical_and((image1_means > 0), (image2_means > 0))\n",
    "    I1m_trunc = image1_means[IND1, ...]\n",
    "    I2m_trunc = image2_means[IND1, ...]\n",
    "    \n",
    "    I1m_trunc, I2m_trunc = rout.remove_outlier(I1m_trunc, I2m_trunc, 0.5, 2)\n",
    "    \n",
    "    # Extract density values from the 2D scatter plot\n",
    "    \n",
    "    I1m_den = I1m_trunc\n",
    "    I2m_den = I2m_trunc\n",
    "    \n",
    "    # Calculate the covariance matrix of the data with outliers removed\n",
    "    cov_matrix = cov(I1m_den, I2m_den)\n",
    "    \n",
    "    # Calculate the eigenvalues\n",
    "    dA, vA = linalg.eig(cov_matrix)\n",
    "    \n",
    "    # Calculate K and B \n",
    "    # K is based on whichever value is dA is the largest\n",
    "    if (dA[0] > dA[1]): # dA[0] is largest\n",
    "        K = vA[1, 0] / vA[0, 0]\n",
    "    else: # dA[1] is largest\n",
    "        K = vA[1, 1] / vA[0, 1]\n",
    "    B = 2 * mean(I1m_den - I2m_den) / mean(I1m_den + I2m_den)\n",
    "    \n",
    "    return K, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_KB_self_new(deltaS2, deltaC2, directory, Nd_self, bin_size, sparse_lidar_flag):\n",
    "    selffile_data = sio.loadmat(o.spath.join(directory, \"output\", \"self.mat\"))\n",
    "    image1 = selffile_data['I1']\n",
    "    image2 = selffile_data['I2']\n",
    "    lines = int(image1.shape[0])\n",
    "    samples = int(image1.shape[1])\n",
    "    \n",
    "    # Set the S and C parameters to the average S and C plus the delta values\n",
    "    S_param2 = 0.65 + deltaS2\n",
    "    C_param2 = 13 + deltaC2\n",
    "    \n",
    "    # Create gamma and run arc_since for image2\n",
    "    gamma2 - image2.copy()\n",
    "    gamma2 = gamma2 / S_param2\n",
    "    image2 = arc.arc_sinc(gamma2, C_param2)\n",
    "    image2[inan(gamma2)] = nan\n",
    "    \n",
    "    # Partition image into subsections for noise suppression (multi-step process)\n",
    "    # Create M and N which are the number of subsections in each direction\n",
    "    # NC and NY are the subsection dimensions\n",
    "    NX = Nd_self\n",
    "    NY = Nd_self\n",
    "    M = int(fix(lines / NY))\n",
    "    N = int(fix(samples/ NX))\n",
    "    \n",
    "    # Create JM and JN which is te remainder after dividing into subsections\n",
    "    JM = lines % NY\n",
    "    JN = samples % NX\n",
    "    \n",
    "    # Select the portions of images that are within the subsections\n",
    "    image1 = image1[0:lines - JM][:, 0:samples - JN]\n",
    "    image2 = image2[0:lines - JM][:, 0:samples - JN]\n",
    "    \n",
    "    # Split each image into subsections and run mean_wo_nan on each subsection\n",
    "    \n",
    "    # Declare new array to hold subsections averages\n",
    "    image1_means = zeros((M, N))\n",
    "    image2_means = zeros((M, N))\n",
    "    \n",
    "    # Processing image1\n",
    "    # Split image into sections with NY number or rows each\n",
    "    image1_rows = split(image1, M, 0)\n",
    "    for i in range(M):\n",
    "        # Split each section into subsections with NX number of columns in each\n",
    "        row_array = split(image1_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean wihout NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image1_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "            \n",
    "    # Processing image2\n",
    "    # Split image into subsections with NY number of rows in each\n",
    "    image2_rows = split(image2, M, 0)\n",
    "    for i in range(M):\n",
    "        # Split each section into subsections with NX number of columns\n",
    "        row_array = split(image1_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image2_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "    \n",
    "    # Make an array for each image of where mean > 0 for both images\n",
    "    IND1 = logical_and((image1_means > 0), (immage2_means > 0))\n",
    "    I1m_trunc = image1_means[IND1, ...]\n",
    "    I2m_trunc = image2_means[IND1, ...]\n",
    "    \n",
    "    # Remove the overestimate at low height end (usually subject to imperfection of the mask\n",
    "    # over water bodies, farmlands and human activities) and the saturation point over the forested areas due to logging\n",
    "    IND2 - logical_or((I1m_trunc < 5), (I2m_trunc > (mt.pi * C_param2 - 1)))\n",
    "    IND2 = logical_not(IND2);\n",
    "    \n",
    "    \n",
    "    # Call remote_outlier on these cells when there are only a few of lidarsamples that are sparsely distributed\n",
    "    if sparse_lidar_flag == 1:\n",
    "        I1m_trunc = I1m_trunc[IND2, ...]\n",
    "        I2m_trunc = I2m_trunc[IND2, ...]\n",
    "        # Extract density values from the 2D scatter plot\n",
    "        I1m_den, I2m_denespd.extract_scatterplot_density(I1m_trunc, I2m_trunc, bin_size)\n",
    "    else:\n",
    "        I1m_trunc, I2m_trunc = rout.remove_outlier(I1m_trunc, I2m_trunc, 0.5, 2)\n",
    "        I1m_den = I1m_trunc\n",
    "        I2m_den = I2m_trunc\n",
    "    \n",
    "    # Calculate the covariance matrix of the data with outliers removed\n",
    "    cov_matrix = cov(I1m_den, I2m_den)\n",
    "    \n",
    "    # Calculate the eigenvalues\n",
    "    dA, vA = linalg.eig(cov_matrix)\n",
    "    \n",
    "    # Calculate K and B\n",
    "    # K is based on whichever value in dA is the largest\n",
    "    if (dA[0] > dA[1]): # dA[0] is largest\n",
    "        K = vA[1,0] /vA[0,0]\n",
    "    else: # dA[1] is largest\n",
    "        K = vA[1, 1] /vA[0, 1]\n",
    "    B = 2 * mean(I1m_den - I2m_den) / mean(I1m_den + I2m_den)\n",
    "    \n",
    "    return K, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_error_metric(dp, edges, start_scene, link, directory, N_pairwise, N_self):\n",
    "    # make output matrix of zeros\n",
    "    YY = zeros((edges + 1) * 2)\n",
    "    if link.size != 0:\n",
    "        # for each edge run cal_error_metric_pairwise and put the output into YY\n",
    "        for i in range(edges):\n",
    "            R_temp, RMSE_temp = emp.cal_error_metric_pairwise(int(link[i, 0]), int(link[i, 1]), dp[int((2*link[i, 0])-2)], dp[int((2*link[i, 0])-1)], dp[int((2*link[i, 1])-2)], dp[int((2*link[i, 1])-1)], directory, N_pairwise)\n",
    "            YY[2 * i] = R_temp\n",
    "            YY[(2 * i) + 1] = RMSE_temp \n",
    "            \n",
    "    # Run cal_error_metric_self and put output into YY\n",
    "    R_temp, RMSE_temp = ems.cal_error_metric_self(dp[int((2 * start_scene) - 2)], dp[int((2 * start_scene) - 1)], directory, N_self)\n",
    "    YY[(2 * (edges + 1)) - 2] = R_temp\n",
    "    YY[(2 * (edges + 1)) - 1] = RMSE_temp\n",
    "    \n",
    "    # return Y\n",
    "    return YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_error_metric_pairwise(scene1, scen2, deltaS1, deltaC1, deltaS2, deltaC2, directory, N_pairwise):\n",
    "    # Set main file name string as scene1_scene2\n",
    "    file_str = str(scene1) + '_' + str(scene2)\n",
    "    \n",
    "    # Load and read data from .mat file\n",
    "    # Samples and lines are calculated from the shape of the images\n",
    "    selffile_data = sio.loadmat(os.path.join(directory, \"output\", file_str + \".mat\"))\n",
    "    image1 = selffile_data['I1']\n",
    "    image2 = selffile_data['I2']\n",
    "    lines = int(image1.shape[0])\n",
    "    samples = int(image1.shape[1])\n",
    "    \n",
    "    # S and C parameters are the average S and C plus the delta value\n",
    "    S_param1 = 0.65 + deltaS1\n",
    "    S_param2 = 0.65 + deltaS2\n",
    "    C_param1 = 13 + deltaC1\n",
    "    C_param2 = 13 + deltaC2\n",
    "    \n",
    "    # Create gamma and run arc_since for image1\n",
    "    gamma1 = image1.copy()\n",
    "    gamma1 = gamma1 / S_param1\n",
    "    image1 = arc.arc_sinc(gamma1, C_param1)\n",
    "    image1[isnan(gamma1)] = nan\n",
    "    \n",
    "    # Create gamma and run arc_since for image2\n",
    "    gamma2 = image2.copy()\n",
    "    gamma2 = gamma2 / S_param2\n",
    "    image2 = arc.arc_sinc(gamma2, C_param2)\n",
    "    image2[isnan(gamma2)] = nan\n",
    "    \n",
    "    # Partition image into subsections for noise suppression (multi-step process)\n",
    "    # Create M and N which are the number of subsections in each direction; fix() rounds towards zero\n",
    "    # NX and NY are the subsection dimensions\n",
    "    NX = N_pairwise\n",
    "    NY = N_pairwise\n",
    "    M = int(fix(lines / NY))\n",
    "    N = int(fix(samples / NX))\n",
    "    \n",
    "    # Create JM and JN, which is the remainder after dividing into subsections\n",
    "    JM = lines % NY\n",
    "    JN = samples % NX\n",
    "    \n",
    "    # Select the portions of images that are within the subsections\n",
    "    image1 = image1[0:lines - JM][:, 0:samples - JN]\n",
    "    image2 = image2[0:lines - JM][:, 0:samples - JN]\n",
    "    \n",
    "    # Split each image into subsections and run mean_wo_nan on each subsection\n",
    "    \n",
    "    # Declare new arrays to hold the subsection averages\n",
    "    image1_means = zeros((M, N))\n",
    "    image2_means = zeros((M, N))\n",
    "    \n",
    "    # Processing image1\n",
    "    # Split image into subsections with NY number of rows in each\n",
    "    image1_rows = split(image1, M, 0)\n",
    "    for i in range(M):\n",
    "        # split each section into subsections with NX number of columns in each\n",
    "        row_array = split(image1_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN andsave the value in another array\n",
    "        for j in range(N):\n",
    "            image1_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "    \n",
    "    # Processing image2\n",
    "    # Split image into sections with NY number of rows in each\n",
    "    image2_rows = split(image2, M, 0)\n",
    "    for i in range(M):\n",
    "        # split each section into subsections with NX number of columns in each\n",
    "        row_array = split(image2_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN an dsave the values in another array\n",
    "        for j in range(N):\n",
    "            image2_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "    \n",
    "    # Make an array for each image of where mean > 0 for both images\n",
    "    IND1 = logical_and((image1_means > 0), (image2_means > 0))\n",
    "    I1m_trunc = image1_means[IND1, ...]\n",
    "    I2m_trunc = iamge2_means[IND1, ...]\n",
    "    \n",
    "    R = corrcoef(I1m_trunc, I2m_trunc)\n",
    "    R = R[0,1]\n",
    "    RMSE = sqrt(sum((I1m_trunc - I2m_trunc)**2)/I1m_trunc.size)\n",
    "    \n",
    "    # Export the pair of heights for future scatter plot\n",
    "    filename = file_str + \"_I1andI2.json\"\n",
    "    R_RMSE_file = open(os.path.join(directory, \"output\", filename), 'w')\n",
    "    json.dump([I1m_trunc.tolist(), I2m_trunc.tolist()], R_RMSE_file)\n",
    "    R_RMSE_file.close()\n",
    "    \n",
    "    return R, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_error_metric_self(deltaS2, deltaC2, directory, N_self):\n",
    "    # load and read data from .mat file\n",
    "    # Samples and lines are calculated from the shape of the images\n",
    "    selffile_data = sio.loadmat(os.path.join(directory, \"output\", \"self.mat\"))\n",
    "    image1 = selffile_data['I1']\n",
    "    image2 = selffile_data['I2']\n",
    "    lines = int(image1.shape[0])\n",
    "    samples = int(image1.shape[1])\n",
    "    \n",
    "    # set the S and C parameters to the average S and C plus the delta values\n",
    "    S_param2 = 0.65 + deltaS2\n",
    "    C_param2 = 13 + deltaC2\n",
    "    \n",
    "    # Create gamma and run arc_since for image2\n",
    "    gamma2 = image2.copy()\n",
    "    gamma2 = gamm2 / S_param2\n",
    "    image2 = arc.arc_sinc(gamma2, C_param2)\n",
    "    image2[isnan(gamma2)] = nan\n",
    "    \n",
    "    # Partition image into subsections for noise suppression (multi-step process)\n",
    "    # Create M and N which are the number of subsections in each direction\n",
    "    # NX and NY aer the subsection dimensions \n",
    "    NX = N_self\n",
    "    NY = N_eslf\n",
    "    M = int(fix(lines / NY))\n",
    "    N = int(fix(samples / NX))\n",
    "    \n",
    "    # Create JM and JN, which is the remainder after dividing into subsections\n",
    "    JM = lines % NY\n",
    "    JN = samples % NX\n",
    "    \n",
    "    # Select the portions of images that are within the subsections\n",
    "    image1 = image1[0:lines - JM][:, 0:samples - JN]\n",
    "    image2 = image2[0:lines - JM][:, 0:samlpes - JN]\n",
    "    \n",
    "    # Split each image into subsections and run mean_wo_nan on each subsection\n",
    "    \n",
    "    # Declare new array to hold subsection averages\n",
    "    image1_means = zeros((M, N))\n",
    "    image2_means = zeros((M, N))\n",
    "    \n",
    "    # Processing image1\n",
    "    # Split image into sections with NY number of rows in each\n",
    "    image1_rows = split(image1, M, 0)\n",
    "    for i in range(M):\n",
    "        # split each section subsections with NX number of columns in each \n",
    "        row_array = split(iamge_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image1_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "            \n",
    "    # Processing image2\n",
    "    # Split image into sections with NY number of rows in each\n",
    "    image2_rows = split(image2_rows[i], N, 1)\n",
    "    for i in range(M):\n",
    "        # split each section into subsections with NX number o fcolumns in each\n",
    "        for j in range(N):\n",
    "            image2_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "    \n",
    "    # Make an array for each imaeg where mean > 0 for both images\n",
    "    IND1 = logical_and((image1_means > 0), (image2_means > 0))\n",
    "    I1m_trunc = image1_means[IND1, ...]\n",
    "    I2m_trunc = image2_means[IND1, ...]\n",
    "    \n",
    "    R = corrcoef(I1m_trunc, I2m_trunc)\n",
    "    R = R[0, 1]\n",
    "    RMSE = sqrt(sum((I1m_trunc-I2m_trunc)**2)/I1m_trunc.size)\n",
    "    \n",
    "    filename = \"self_I1andI2.json\"\n",
    "    R_RMSE_file = open(os.path.join(directory, \"output\", filename), 'w')\n",
    "    json.dump([I1m_trunc.tolist(), I2m_trunc.tolist()], R_RMSE_file)\n",
    "    R_RMSE_file.close()\n",
    "    \n",
    "    return R, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mosaic(directory, mosaicfile): \n",
    "    \n",
    "    print(time.strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    # Making list of files\n",
    "    listofffile = ''\n",
    "    iterdir = [f for f in os.listdir('.') if os.path.isdir(f) and f.startswith('f')]\n",
    "    \n",
    "    for num, val in enumerate(iterdir):\n",
    "        os.chdir(os.path.join(directory, val))\n",
    "        tiffile = [f for f in os.listdir('.') if f.endswith('fsh.tif')]\n",
    "        abspth = str(pathlib.Path(tiffile).absolute())\n",
    "        listoffiles = listoffiles + ' ' + abspth\n",
    "    \n",
    "    os.chdir(directory)\n",
    "    print(directory)\n",
    "    subprocess.getoutput('gdalbuildvrt -seperate -srcnodata 255 -overwrite ' + os.path.join(directory, 'mosaic.vrt') + listoffiles)\n",
    "    subprocess.getoutput('gdal_translate -of GTiff -a_nodata 255 ' + os.path.join(directory, 'mosaic.vrt') + ' ' + os.path.join(dirextory, 'mosaic.tif'))\n",
    "    \n",
    "    # Load mosaic.tif and associated parameters - .tif\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    drier.Register()\n",
    "    img = gdal.Open(os.path.join(directory, 'mosaic.tif'))\n",
    "    ref_data = array(img.ReadAsArray())\n",
    "    refgeotrans = img.GetGeoTransform()\n",
    "    corner_lon = refgeotrans[0]\n",
    "    post_lon = refgeotrans[1]\n",
    "    corner_lat = refgeotrans[3]\n",
    "    post_lat = refgeotrans[5]\n",
    "    geo_width = img.RasterXSize\n",
    "    geo_lines = img.RasterYSize\n",
    "    \n",
    "    ######### average all of the overlappingpixels at the same area\n",
    "    ref_data = single(ref_data)\n",
    "    ref_data[ref_data==255] = NaN\n",
    "    avg = nanmean(ref_data, axis = 0)\n",
    "    avg[isnan(avg)] = 255\n",
    "    \n",
    "    ######### Create the final GeoTiff\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    outRaster = driver.Create(os.path.join(directory, mosaicfile), geo_width, geo_lines)\n",
    "    outRaster.SetGeoTransform([corner_lon, post_lon, 0, corner_lat, 0, post_lat])\n",
    "    outband = outRaster.GetRasterBand(1)\n",
    "    \n",
    "    outband.WriteArray(avg)\n",
    "    outRasterSRS = osr.SpatialReference()\n",
    "    outRasterSRS.ImportFromEPSG(4326)\n",
    "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "    outband.FlushCache()\n",
    "    \n",
    "    print(time.strftime(\"%H:%M:%S\"))\n",
    "    print((\"Final mosaic generation done!!!\"))\n",
    "    \n",
    "parser = argparse.ArgumentParser(description = \"Create final mosaic map of forest stand height\")\n",
    "parser.add_argument('directory', type = str, help = 'the same root directory as forest_stand_height.py executes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
