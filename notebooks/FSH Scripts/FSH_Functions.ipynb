{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Stand Height Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "import math as mt\n",
    "import json\n",
    "import simplekml\n",
    "from PIL import Image\n",
    "import os.path\n",
    "import scipy.io as sio\n",
    "from scipy.interpolate import griddata\n",
    "import subprocess\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import argparse\n",
    "import pdb\n",
    "from osgeo import gdal, osr\n",
    "import string\n",
    "import pathlib\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gamma\n",
    "from mpmath import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crop_ISCE:\n",
    "    xmlfile = \"resampleOnlyImage.amp.xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    size_array = np.array([])\n",
    "    for size in root.iter('property'):\n",
    "        if size.items()[0][1] == 'size':\n",
    "            size_array = np.append(size_array, int(size.find('value').text))\n",
    "    width = size_array[0]\n",
    "    length = size_array[1]\n",
    "    \n",
    "    nanval = 0\n",
    "    \n",
    "    # Read amp files in radar coordinates\n",
    "    amp_file = np.fromfile(\"resampOnlyImage.amp\", dtype = 'complex64')\n",
    "    inty = amp_file.reshape((length, width))\n",
    "    \n",
    "    # Creating empty array for cropped square list\n",
    "    \n",
    "    inty[:176,:] = nanval\n",
    "    inty[5488:,:] = nanval\n",
    "    inty[:,:163] = nanval\n",
    "    inty[:,4846:] = nanval\n",
    "    \n",
    "    # Write output files\n",
    "    inty.tofile(\"resampOnlyImage.amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Crop_ROIPAC(directory, date1, date2):\n",
    "    # Extract ROI_PAC parameters\n",
    "    amp_rsc_file = date1 + \"-\" + date2 + \"-\" + \"_2rlk.amp.rsc\"\n",
    "    width = int(rrd.read_rsc_data(amp_rsc_file, directory, \"WIDTH\"))\n",
    "    length = int(rrd.read_rsc_data(amp_rsc_file, directory, \"FILE_LENGTH\"))\n",
    "    fullwidth = width*2\n",
    "    nanval = 0\n",
    "    \n",
    "    # Readcpr files in radar coordinates\n",
    "    cor_file = np.fromfile(directory + date1 + \"-\" + date2 + \"_2rlks.cor\", dtype = \"f4\", count = length*fullwidth)\n",
    "    corr = cor_file.reshape((length, fullwidth))\n",
    "    mag = corr[:, 0:width]\n",
    "    phs = corr[:.width:fullwidth]\n",
    "    \n",
    "    # Read amp files in radar coordinates\n",
    "    amp_file = np.fromfile(directory + date1 + \"-\" + date2 + \"_2rlks.amp\", dtype = 'complex64')\n",
    "    inty = amp_file.reshape((length, width))\n",
    "    \n",
    "    # Creating empty array for cropped square list\n",
    "    mag[:638, :] = nanval\n",
    "    mag[3288:, :] = nanval\n",
    "    mag[:,:84] = nanval\n",
    "    mag[:,2418:] = nanval\n",
    "    \n",
    "    phs[:638, :] = nanval\n",
    "    phs[3288:, :] = nanval\n",
    "    phs[:,:84] = nanval\n",
    "    phs[:,2418:] = nanval\n",
    "    \n",
    "    inty[:638, :] = nanval\n",
    "    inty[3288:, :] = nanval\n",
    "    inty[:,:84] = nanval\n",
    "    inty[:,2418:] = nanval\n",
    "    \n",
    "    # Writing values\n",
    "    c_out[:, 0:width] = mag\n",
    "    c_out[:,width:fullwidth] = phs\n",
    "    \n",
    "    # Write output files\n",
    "    cx = c_out.astype('f4')\n",
    "    cx.tofile(directory + date1 + \"-\" + date2 + \"_2rlks_fix.cor\")\n",
    "    inty.tofile(directory + date1 + \"-\" + date2 + \"_2rlks_fix.amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arc_sinc(x, C_param):\n",
    "    # Get rid of extreme values by set all values where x > 1 equal to 1, and x < 0 equal to 0\n",
    "    x[(x > 1)] = 1\n",
    "    x[(x < 0)] = 0\n",
    "    \n",
    "    # Create array of increments between 0 and pi of size pi/100\n",
    "    XX = linspace(0, math.pi, num = 100, endpoint = True)\n",
    "    \n",
    "    # Set the first value of XX to eps to avoid division by zero issues\n",
    "    XX[0] = spacing(1)\n",
    "    \n",
    "    # Calculate sinc for for XX and save it to YY\n",
    "    ## YY - sinc(XX / math.pi)\n",
    "    YY = np.sin(XX) / XX\n",
    "    \n",
    "    # Reset the first value of XX to zero and the first value of YY to the corresponding output\n",
    "    XX[0] = 0\n",
    "    YY[0] = 1\n",
    "    \n",
    "    # Set the last value of YY to 0 to avoid NaN issues\n",
    "    YY[-1] = 0\n",
    "    \n",
    "    # Flip XX and YY left to right\n",
    "    YY = YY[::-1]\n",
    "    XX = XX[::-1]\n",
    "    \n",
    "    # Run interpolation\n",
    "    # XX and YY are your original values, x is the query value, and y is the interpolated values that correspond to x\n",
    "    y = interp_func(x)\n",
    "    \n",
    "    # Set all values in y less than 0 equal to 0\n",
    "    y[(y < 0)] = 0\n",
    "    # return y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters are the numbers of scenes, edges, start scene, iterations, the input/output file directory, \n",
    "    # averaging numbers in lat and lon for \"self\" and \"pairwise\" fitting, bin_size for density calculation in scatter plot fitting, \n",
    "    # flag for sparse data cloud filtering.\n",
    "def auto_mosaicking_new(scenes, edges, start_scene, N, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse):\n",
    "    # Set average S and C parameters (0 < s < 1, 0 < c < 20 so s = 0.65, and c = 13)\n",
    "    avg_S = 0.65\n",
    "    avg_C = 13\n",
    "    \n",
    "    # Create avg_dp matric, and fill with average S and C parameters\n",
    "    avg_dp = zeros(scenes * 2)\n",
    "    put(avg_dp, range(0, scenes * 2, 2), avg_S)\n",
    "    put(avg_dp, range(1, scenes * 2, 2), avg_C)\n",
    "    \n",
    "    # Create the dp matrix\n",
    "    # the difference of the avg and the initial SC values OR all the zeros (avg)\n",
    "    dp = zeros(scenes * 2)\n",
    "    \n",
    "    # Intialize target matrix nd fill with K = 1, B = 0\n",
    "    target_KB = zeros((edges + 1) * 2)\n",
    "    put(target_KB, range(0, (edges + 1) * 2, 2), 1)\n",
    "    \n",
    "    # Run cal_KB()\n",
    "    Y = cKB.cal_KB(dp, edges, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "    \n",
    "    # Calculate the residual for cal_KB - target\n",
    "    res = sum((Y - target_KB)**2)\n",
    "    \n",
    "    # Save dp and the residuals as the first iteration output file (using JSON)\n",
    "    iter_file = open(os.path.join(dirextory, \"output\", \"SC_0_iter.json\"), 'w')\n",
    "    json.dump([dp.tolist(), res], iter_file)\n",
    "    iter_file.close()\n",
    "    \n",
    "    # For the rest of the iterations run ls_deltaSC() and save to output file (using JSON)\n",
    "    for i in range(1, N + 1, 2): # This will run from i = 1 to i = N\n",
    "        [dp, res] = lSC.ls_deltaSC(dp, edges, scenes, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "        print(\"%d iterations completed!\\n\" % i)\n",
    "        print(time.strftime(\"%H:%M:%S\"))\n",
    "        filename = \"SC_%d_iter.json\" % i\n",
    "        iter_file = open(os.path.join(directory, \"output\", filename), 'w')\n",
    "        json.dump([dp.tolist(), res], iter_file)\n",
    "        iter_file.close()\n",
    "        \n",
    "    print(\"auto_mosaicking_new finsihed at \" + (time.strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tree_height_many(scenes, flagfile, directory, numLooks, noiselevel, flag_proc, flag_grad):\n",
    "    # for each scene name the file, run auto_tree_height_single and save the output to a .json file\n",
    "    for i in range(scenes):\n",
    "        \n",
    "        # Get the scene data and set the file name and image folder name (f#_o# where # is the frame and orbit numbers, respectively)\n",
    "        scene_data = fsf.flag_scene_file(flagfile, i + 1, directory) # 0 vs1 indexing\n",
    "        filename = scene_data[1]\n",
    "        image_folder = \"f\" + scene_data[4] + \"_o\" + scene_data[5] + \"/\"\n",
    "        \n",
    "        # Run auto_tree_height_single\n",
    "        if flag_proc == 0:\n",
    "            ######## ROI_PAC results\n",
    "            file_date = athsR.auto_tree_height_single_ROIPAC(directory + image_folder, scene_data[2], scene_data[3], numLooks, noiselevel, flag_grad)\n",
    "        elif flag_proc == 1:\n",
    "            ######## ISCE results\n",
    "            file_data = athsI.auto_tree_height_single_ISCE(directory + image_folder, scene_data[2], scene_data[3], numLooks, noiselevel, flag_grad)\n",
    "        else:\n",
    "            print(\"Invalid processor provided\")\n",
    "            \n",
    "        linkfile = directory + image_folder + filename + '_orig.mat'\n",
    "        \n",
    "    sio.savemat(linkfile, {'corr_vs':file_data[0], 'ks':file_data[1], 'coords':file_data[2]})\n",
    "    \n",
    "    # Write geodata to a text file (4th - 9th values in file_data) \n",
    "    geofile = open(directory + image_folder + filename + \"_geo.txt\", \"w\")\n",
    "    geofile.write(\"width: %d \\n\" % file_data[3])\n",
    "    geofile.write(\"nlines: %d \\n\" % file_data[4])\n",
    "    geofile.write(\"corner_lat: %f\" % file_data[5])\n",
    "    geofile.write(\"corner_lon: %f\" % file_data[6])\n",
    "    geofile.write(\"post_lat: %f\" % file_data[7])\n",
    "    geofile.write(\"post_lon: %f\" % file_data[8])\n",
    "    geofile.close()\n",
    "    \n",
    "    print (\"auto_tree_height_many finished at \" + (time.strftime(\"H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_tree_height_single_ISCE(directory, date1, date2, numLooks, noiselevel, flag_grad):\n",
    "    # Extract ISCE parameters\n",
    "    xmlfile = subprocess.getoutput('find ' + directory + 'int_' + date1 + '_' + date2 + '/ -name *Proc.xml')\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    root_tag = root.tag\n",
    "    \n",
    "    range_pixel_res = float(root.findall(\"./master/instrument/range_pixel_size\")[0].text)\n",
    "    llambda = float(root.findall(\"./master/instrument/radar_wavelength\")[0].text)\n",
    "    try:\n",
    "        first_range = float(root.findall(\"./runTopo/inputs/range_first_sample\")[0].text)\n",
    "    except:\n",
    "        first_range = float(root.findall(\"./runTopo/inputs/RANGE_FIRST_SAMPLE\")[0].text)\n",
    "    try:\n",
    "        num_range_bin = int(root.findall(\"./runTopo/inputs/width\")[0].text)\n",
    "    except:\n",
    "        num_range_bin = int(root.findall(\"./runTopo/inputs/WIDTH\")[0].text)\n",
    "    try:\n",
    "        num_range_looks = int(root.findall(\"./runTopo/inputs/number_range_looks\")[0].text)\n",
    "    except:\n",
    "        num_range_looks = int(root.findall(\"./runTopo/inputs/NUMBER_RANGE_LOOKS\")[0].text)\n",
    "    center_range = first_range + (num_range_bin/2-1)*range_pixel_res*num_range_looks\n",
    "    incid_angle = float(root.findall(\"./master/instrument/incidence_angle\")[0].text)\n",
    "    baseline_top = float(root.findall(\"./baseline/perp_baseline_top\")[0].text)\n",
    "    baseline_bottom = float(root.findall(\"./baseline/perp_baseline_bottom\")[0].text)\n",
    "    baseline = (baseline_bottom+baseline_top)/2\n",
    "    \n",
    "\n",
    "    xmlfile = directory+\"int_\"+date1+\"_\"+date2+\"/topophase.cor.geo.xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    delta_array = np.array([])\n",
    "    start_array = np.array([])\n",
    "    size_array = np.array([], dtype=np.int32)\n",
    "    for size in root.iter('property'):\n",
    "        if size.items()[0][1] == 'size':\n",
    "            size_array = np.append(size_array, int(size.find('value').text))\n",
    "    for delta_val in root.iter('property'):\n",
    "        if delta_val.items()[0][1] == 'delta':\n",
    "            delta_array = np.append(delta_array, float(delta_val.find('value').text))\n",
    "    for start_val in root.iter('property'):\n",
    "        if start_val.items()[0][1] == 'startingvalue':\n",
    "            start_array = np.append(start_array, float(start_val.find('value').text))\n",
    "    end_array = start_array + size_array * delta_array\n",
    "    north = max(start_array[1],end_array[1])\n",
    "    south = min(start_array[1],end_array[1])\n",
    "    east = max(start_array[0],end_array[0])\n",
    "    west = min(start_array[0],end_array[0])\n",
    "    coords = [north, south, west, east]\n",
    "    geo_width = size_array[0]\n",
    "    geo_nlines = size_array[1]\n",
    "    corner_lat = north\n",
    "    corner_lon = west\n",
    "    step_lat = delta_array[1]\n",
    "    step_lon = delta_array[0]\n",
    "\n",
    "    xmlfile = directory+\"int_\"+date1+\"_\"+date2+\"/resampOnlyImage.amp.geo.xml\"\n",
    "    tree = ET.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    delta_array = np.array([])\n",
    "    start_array = np.array([])\n",
    "    size_array = np.array([], dtype=np.int32)\n",
    "    for size in root.iter('property'):\n",
    "        if size.items()[0][1] == 'size':\n",
    "            size_array = np.append(size_array, int(size.find('value').text))\n",
    "    if (size_array[0]<geo_width)|(size_array[1]<geo_nlines):\n",
    "        for delta_val in root.iter('property'):\n",
    "            if delta_val.items()[0][1] == 'delta':\n",
    "                delta_array = np.append(delta_array, float(delta_val.find('value').text))\n",
    "        for start_val in root.iter('property'):\n",
    "            if start_val.items()[0][1] == 'startingvalue':\n",
    "                start_array = np.append(start_array, float(start_val.find('value').text))\n",
    "        end_array = start_array + size_array * delta_array\n",
    "        north = max(start_array[1],end_array[1])\n",
    "        south = min(start_array[1],end_array[1])\n",
    "        east = max(start_array[0],end_array[0])\n",
    "        west = min(start_array[0],end_array[0])\n",
    "        coords = [north, south, west, east]\n",
    "        geo_width = size_array[0]\n",
    "        geo_nlines = size_array[1]\n",
    "        corner_lat = north\n",
    "        corner_lon = west\n",
    "        step_lat = delta_array[1]\n",
    "        step_lon = delta_array[0]\n",
    "\n",
    "\n",
    "    # Read geolocated amp and cor files\n",
    "\n",
    "    fid_cor = open(directory + \"int_\"+date1+\"_\"+date2+\"/topophase.cor.geo\", \"rb\")\n",
    "    cor_file = np.fromfile(fid_cor, dtype=np.dtype('<f'))\n",
    "\n",
    "    corr = cor_file.reshape(2*geo_width, -1, order='F')\n",
    "    corr = corr[:,0:geo_nlines]\n",
    "    corr_mag = corr[geo_width:2*geo_width,:]\n",
    "\n",
    "    fid_amp = open(directory + \"int_\"+date1+\"_\"+date2+\"/resampOnlyImage.amp.geo\", \"rb\")\n",
    "    amp_file = np.fromfile(fid_amp, dtype=np.dtype('<f'))\n",
    "    inty = amp_file.reshape(2*geo_width, -1, order='F')\n",
    "    inty = inty[:,0:geo_nlines]\n",
    "    inty1 = inty[::2,:]\n",
    "    inty2 = inty[1::2,:]\n",
    "\n",
    "\n",
    "    # Operations\n",
    "    inty1 = np.power(inty1,2) # Hardcoded based on 2 range looks and 10 azimuth looks\n",
    "    inty2 = np.power(inty2,2)\n",
    "\n",
    "    inty1[inty1 <= 0] = np.NaN\n",
    "    inty2[inty2 <= 0] = np.NaN\n",
    "    corr_mag[corr_mag <= 0] = np.NaN\n",
    "\n",
    "    ####### Noise level for ISCE-processed SAR backscatter power output\n",
    "    if noiselevel == 0.0:\n",
    "        if root_tag[0] == 'i':\n",
    "            ####### ALOS thermal noise level (insarApp)\n",
    "            N1 = 55.5**2\n",
    "            N2 = 55.5**2\n",
    "        elif root_tag[0] == 's':\n",
    "            ####### ALOS thermal noise level (stripmapApp)\n",
    "            N1 = (55.5/81)**2\n",
    "            N2 = (55.5/81)**2\n",
    "        else:\n",
    "            raise Exception(\"invalid *Proc.xml file!!!\")\n",
    "    else:\n",
    "        N1 = noiselevel\n",
    "        N2 = noiselevel\n",
    "\n",
    "\n",
    "    S1 = inty1 - N1\n",
    "    g_th_1 = np.zeros(S1.shape)\n",
    "    g_th_1[S1>N1] = np.sqrt(S1[S1>N1] / (S1[S1>N1] + N1))\n",
    "    g_th_1[np.isnan(S1)] = np.NaN\n",
    "    g_th_1[S1 <= N1] = np.NaN\n",
    "\n",
    "    S2 = inty2-N2\n",
    "    g_th_2 = np.zeros(S2.shape)\n",
    "    g_th_2[S2>N2] = np.sqrt(S2[S2>N2] / (S2[S2>N2] + N2))\n",
    "    g_th_2[np.isnan(S2)] = np.NaN\n",
    "    g_th_2[S2 <= N2] = np.NaN\n",
    "\n",
    "    g_th = g_th_1 * g_th_2\n",
    "\n",
    "    corr_mag[corr_mag<0] = 0\n",
    "    corr_mag[corr_mag>1] = 1\n",
    "    corr_mag = rcb.remove_corr_bias(corr_mag,numLooks)\n",
    "    corr_mag[corr_mag<0] = 0\n",
    "\n",
    "    corr_vs = corr_mag / g_th\n",
    "\n",
    "    # set constants\n",
    "    pi=mt.pi\n",
    "\n",
    "    # correcting geometric decorrelation related to value compensation of ROI result compared to GAMMA. Caused by baseline/other decorrelation\n",
    "    gamma_base = 1 - (2 * mt.fabs(baseline) * mt.cos(incid_angle / 180 * pi) * range_pixel_res / mt.sin(incid_angle / 180 * pi) / llambda / center_range)\n",
    "    gamma_geo = gamma_base\n",
    "    corr_vs = corr_vs / gamma_geo\n",
    "    corr_vs[corr_vs>1] = 1\n",
    "\n",
    "    ##### Simple Radiometric correction of the coherences\n",
    "    if flag_grad == 1:\n",
    "        y = np.linspace(1, geo_width, geo_width)\n",
    "        x = np.linspace(1, geo_nlines, geo_nlines)\n",
    "        [X, Y] = np.meshgrid(x, y)\n",
    "        A = np.vstack([X[~np.isnan(corr_vs)], Y[~np.isnan(corr_vs)], np.ones(np.size(corr_vs[~np.isnan(corr_vs)]))]).T\n",
    "        coeff = np.linalg.lstsq(A, corr_vs[~np.isnan(corr_vs)])[0]\n",
    "        corr_vs = corr_vs - X*coeff[0] - Y*coeff[1]\n",
    "        corr_vs[corr_vs>1] = 1\n",
    "        corr_vs[corr_vs<0] = 0\n",
    "\n",
    "    kz = -2 * pi * 2 / llambda / center_range / mt.sin(incid_angle/180*pi) * baseline\n",
    "    kz = mt.fabs(kz)\n",
    "\n",
    "    # Return corr_vs, kz, coords\n",
    "    return corr_vs, kz, coords, geo_width, geo_nlines, corner_lat, corner_lon, step_lat, step_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rsc_data(filename, directory, param):\n",
    "    # set default output value\n",
    "    result = -1\n",
    "    \n",
    "    # Set filenmae for file to be searched\n",
    "    rsc_file = dirextory + filenmae\n",
    "    \n",
    "    # Read parameters from file\n",
    "    for line in open(rsc_file):\n",
    "        if line.startswith(param):\n",
    "            result = float(line.strip().split()[1])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_KB(dp, edges, start_scene, link, directory, Nd_pairwise, Nd_self, bin_size .flag_sparse):\n",
    "    \n",
    "    # make output matric of zeros \n",
    "    YY = zeros((edges + 1) * 2)\n",
    "    if link.size != 0:\n",
    "        # for each edge run cal_KB_pairwise_new an put theoutput into YY\n",
    "        fir i in range(edges):\n",
    "            k_temp, b_temp = kbp.cal_KB_pairwise_new(int(link[i, 0]), int(link[i, 1]), dp[int((2*link[i, 0])-2)], dp[int((2*link[i, 0])-1)], dp[int((2*link[i, 1])-2)], dp[int((2*link[i, 1])-1)], directory, Nd_pairwise, bin_size)\n",
    "            YY[2 * i] = k_temp\n",
    "            YY[(2 * i)] = b_temp\n",
    "            \n",
    "        # run cal_KB_self_new and put output into YY\n",
    "        \n",
    "        k_temp, b_temp = kbs.cal_KB_self_new(dp[int((2 * start_scene) - 2)], dp[int((2 * start_scene) - 1)], directory, Nd_self, bin_size, flag_sparse)\n",
    "        YY[(2 * (edges + 1)) - 2] = k_temp\n",
    "        YY[(2 * (edges + 1)) - 1] = b_temp\n",
    "        \n",
    "        # Return Y\n",
    "        return YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_KB_pairwise_new(scene1, scene2, deltaS1, deltaC1, deltaS2, deltaC2, directory, Nd_pairwise, bin_size):\n",
    "    # Set main file name string as scene1_scene2\n",
    "    file_str == str(scene1) + '_' + str(scene2)\n",
    "    \n",
    "    # Load and read data from .mat file\n",
    "    # Samples and lines are calculated from the shape of the images\n",
    "    selffile_data = sio.loadmat(directory + \"output/\" + file_str + \".mat\")\n",
    "    image1 = selffile_data['I1']\n",
    "    image2 = selffile_data['I2']\n",
    "    lines = int(image1.shape[0])\n",
    "    samples = int(image1.shape[1])\n",
    "    \n",
    "    # S and C parameters are the average S and C plus the delta value\n",
    "    S_param1 = 0.65 + deltaS1\n",
    "    S_param2 = 0.65 + deltaS2\n",
    "    C_param1 = 13 + deltaC1\n",
    "    C_param2 = 13 + deltaC2\n",
    "    \n",
    "    # Create gamma and ren arc_since for image1\n",
    "    gamm1 = image1.copy()\n",
    "    gamma1 = gamma1 / S_param1\n",
    "    image1 = arc.arc_since(gamma1, C_param1)\n",
    "    image1[isnan(gamma1)] = nan\n",
    "    \n",
    "    # Create gamma and run arc_since for image2\n",
    "    gamma2 = image2.copy()\n",
    "    gamma2 = gamma2 / S_param2\n",
    "    image2 = arc.arc_since(gamma2, C_param2)\n",
    "    image2[isnan(gamma2)] = nan\n",
    "    \n",
    "    # Partition image into subsections for noise suppression (multi-step process)\n",
    "    # Create M and N which are the number of subsections in each direction; fix() rounds towards zero\n",
    "    # NX and NY are the subsection dimensions\n",
    "    NX = Nd_pairwise\n",
    "    NY = Nd_pairwise\n",
    "    M = int(fix(lines / NY))\n",
    "    N = int(fix(samples / NX))\n",
    "    \n",
    "    # Create JM and JN which is the remainder adter dividing into subsections\n",
    "    JM = lines % NY\n",
    "    JN = samples % NX\n",
    "    \n",
    "    # Select the portion of images that are within the subsections\n",
    "    image1 = image1[0:lines - JM][:, 0:samples - JN]\n",
    "    image2 = image2[0:lines - JM][:, 0:samples - JN]\n",
    "    \n",
    "    # Split each image into subsections and run mean_wo_nan on each subsection\n",
    "    \n",
    "    # Declare new arrays to hold the subsection averages\n",
    "    image1_means = zeros((M, N))\n",
    "    image2_means = zeros((M, N))\n",
    "    \n",
    "    # Processing image1\n",
    "    # Split image into subsections with NY of rows in each\n",
    "    \n",
    "    image1_rows = split(image1, M, 0)\n",
    "    for i in range(M):\n",
    "        # Split each section into subsections with NX number of columns in each\n",
    "        row_array_split(image1_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean with NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image1_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "            \n",
    "    # Processing image2\n",
    "    # split image into subsections with NY number of rows in each\n",
    "    image2_rows = split(image2, M, 0)\n",
    "    for i in range(M):\n",
    "        # split each section into subsections with NX number of columns ineach\n",
    "        row_array = split(image2_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image2_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "            \n",
    "    # Make an array for each image of where mean > 0 for both images\n",
    "    IND1 = logical_and((image1_means > 0), (image2_means > 0))\n",
    "    I1m_trunc = image1_means[IND1, ...]\n",
    "    I2m_trunc = image2_means[IND1, ...]\n",
    "    \n",
    "    I1m_trunc, I2m_trunc = rout.remove_outlier(I1m_trunc, I2m_trunc, 0.5, 2)\n",
    "    \n",
    "    # Extract density values from the 2D scatter plot\n",
    "    \n",
    "    I1m_den = I1m_trunc\n",
    "    I2m_den = I2m_trunc\n",
    "    \n",
    "    # Calculate the covariance matrix of the data with outliers removed\n",
    "    cov_matrix = cov(I1m_den, I2m_den)\n",
    "    \n",
    "    # Calculate the eigenvalues\n",
    "    dA, vA = linalg.eig(cov_matrix)\n",
    "    \n",
    "    # Calculate K and B \n",
    "    # K is based on whichever value is dA is the largest\n",
    "    if (dA[0] > dA[1]): # dA[0] is largest\n",
    "        K = vA[1, 0] / vA[0, 0]\n",
    "    else: # dA[1] is largest\n",
    "        K = vA[1, 1] / vA[0, 1]\n",
    "    B = 2 * mean(I1m_den - I2m_den) / mean(I1m_den + I2m_den)\n",
    "    \n",
    "    return K, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_KB_self_new(deltaS2, deltaC2, directory, Nd_self, bin_size, sparse_lidar_flag):\n",
    "    selffile_data = sio.loadmat(o.spath.join(directory, \"output\", \"self.mat\"))\n",
    "    image1 = selffile_data['I1']\n",
    "    image2 = selffile_data['I2']\n",
    "    lines = int(image1.shape[0])\n",
    "    samples = int(image1.shape[1])\n",
    "    \n",
    "    # Set the S and C parameters to the average S and C plus the delta values\n",
    "    S_param2 = 0.65 + deltaS2\n",
    "    C_param2 = 13 + deltaC2\n",
    "    \n",
    "    # Create gamma and run arc_since for image2\n",
    "    gamma2 - image2.copy()\n",
    "    gamma2 = gamma2 / S_param2\n",
    "    image2 = arc.arc_sinc(gamma2, C_param2)\n",
    "    image2[inan(gamma2)] = nan\n",
    "    \n",
    "    # Partition image into subsections for noise suppression (multi-step process)\n",
    "    # Create M and N which are the number of subsections in each direction\n",
    "    # NC and NY are the subsection dimensions\n",
    "    NX = Nd_self\n",
    "    NY = Nd_self\n",
    "    M = int(fix(lines / NY))\n",
    "    N = int(fix(samples/ NX))\n",
    "    \n",
    "    # Create JM and JN which is te remainder after dividing into subsections\n",
    "    JM = lines % NY\n",
    "    JN = samples % NX\n",
    "    \n",
    "    # Select the portions of images that are within the subsections\n",
    "    image1 = image1[0:lines - JM][:, 0:samples - JN]\n",
    "    image2 = image2[0:lines - JM][:, 0:samples - JN]\n",
    "    \n",
    "    # Split each image into subsections and run mean_wo_nan on each subsection\n",
    "    \n",
    "    # Declare new array to hold subsections averages\n",
    "    image1_means = zeros((M, N))\n",
    "    image2_means = zeros((M, N))\n",
    "    \n",
    "    # Processing image1\n",
    "    # Split image into sections with NY number or rows each\n",
    "    image1_rows = split(image1, M, 0)\n",
    "    for i in range(M):\n",
    "        # Split each section into subsections with NX number of columns in each\n",
    "        row_array = split(image1_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean wihout NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image1_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "            \n",
    "    # Processing image2\n",
    "    # Split image into subsections with NY number of rows in each\n",
    "    image2_rows = split(image2, M, 0)\n",
    "    for i in range(M):\n",
    "        # Split each section into subsections with NX number of columns\n",
    "        row_array = split(image1_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image2_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "    \n",
    "    # Make an array for each image of where mean > 0 for both images\n",
    "    IND1 = logical_and((image1_means > 0), (immage2_means > 0))\n",
    "    I1m_trunc = image1_means[IND1, ...]\n",
    "    I2m_trunc = image2_means[IND1, ...]\n",
    "    \n",
    "    # Remove the overestimate at low height end (usually subject to imperfection of the mask\n",
    "    # over water bodies, farmlands and human activities) and the saturation point over the forested areas due to logging\n",
    "    IND2 - logical_or((I1m_trunc < 5), (I2m_trunc > (mt.pi * C_param2 - 1)))\n",
    "    IND2 = logical_not(IND2);\n",
    "    \n",
    "    \n",
    "    # Call remote_outlier on these cells when there are only a few of lidarsamples that are sparsely distributed\n",
    "    if sparse_lidar_flag == 1:\n",
    "        I1m_trunc = I1m_trunc[IND2, ...]\n",
    "        I2m_trunc = I2m_trunc[IND2, ...]\n",
    "        # Extract density values from the 2D scatter plot\n",
    "        I1m_den, I2m_denespd.extract_scatterplot_density(I1m_trunc, I2m_trunc, bin_size)\n",
    "    else:\n",
    "        I1m_trunc, I2m_trunc = rout.remove_outlier(I1m_trunc, I2m_trunc, 0.5, 2)\n",
    "        I1m_den = I1m_trunc\n",
    "        I2m_den = I2m_trunc\n",
    "    \n",
    "    # Calculate the covariance matrix of the data with outliers removed\n",
    "    cov_matrix = cov(I1m_den, I2m_den)\n",
    "    \n",
    "    # Calculate the eigenvalues\n",
    "    dA, vA = linalg.eig(cov_matrix)\n",
    "    \n",
    "    # Calculate K and B\n",
    "    # K is based on whichever value in dA is the largest\n",
    "    if (dA[0] > dA[1]): # dA[0] is largest\n",
    "        K = vA[1,0] /vA[0,0]\n",
    "    else: # dA[1] is largest\n",
    "        K = vA[1, 1] /vA[0, 1]\n",
    "    B = 2 * mean(I1m_den - I2m_den) / mean(I1m_den + I2m_den)\n",
    "    \n",
    "    return K, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_error_metric(dp, edges, start_scene, link, directory, N_pairwise, N_self):\n",
    "    # make output matrix of zeros\n",
    "    YY = zeros((edges + 1) * 2)\n",
    "    if link.size != 0:\n",
    "        # for each edge run cal_error_metric_pairwise and put the output into YY\n",
    "        for i in range(edges):\n",
    "            R_temp, RMSE_temp = emp.cal_error_metric_pairwise(int(link[i, 0]), int(link[i, 1]), dp[int((2*link[i, 0])-2)], dp[int((2*link[i, 0])-1)], dp[int((2*link[i, 1])-2)], dp[int((2*link[i, 1])-1)], directory, N_pairwise)\n",
    "            YY[2 * i] = R_temp\n",
    "            YY[(2 * i) + 1] = RMSE_temp \n",
    "            \n",
    "    # Run cal_error_metric_self and put output into YY\n",
    "    R_temp, RMSE_temp = ems.cal_error_metric_self(dp[int((2 * start_scene) - 2)], dp[int((2 * start_scene) - 1)], directory, N_self)\n",
    "    YY[(2 * (edges + 1)) - 2] = R_temp\n",
    "    YY[(2 * (edges + 1)) - 1] = RMSE_temp\n",
    "    \n",
    "    # return Y\n",
    "    return YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_error_metric_pairwise(scene1, scen2, deltaS1, deltaC1, deltaS2, deltaC2, directory, N_pairwise):\n",
    "    # Set main file name string as scene1_scene2\n",
    "    file_str = str(scene1) + '_' + str(scene2)\n",
    "    \n",
    "    # Load and read data from .mat file\n",
    "    # Samples and lines are calculated from the shape of the images\n",
    "    selffile_data = sio.loadmat(os.path.join(directory, \"output\", file_str + \".mat\"))\n",
    "    image1 = selffile_data['I1']\n",
    "    image2 = selffile_data['I2']\n",
    "    lines = int(image1.shape[0])\n",
    "    samples = int(image1.shape[1])\n",
    "    \n",
    "    # S and C parameters are the average S and C plus the delta value\n",
    "    S_param1 = 0.65 + deltaS1\n",
    "    S_param2 = 0.65 + deltaS2\n",
    "    C_param1 = 13 + deltaC1\n",
    "    C_param2 = 13 + deltaC2\n",
    "    \n",
    "    # Create gamma and run arc_since for image1\n",
    "    gamma1 = image1.copy()\n",
    "    gamma1 = gamma1 / S_param1\n",
    "    image1 = arc.arc_sinc(gamma1, C_param1)\n",
    "    image1[isnan(gamma1)] = nan\n",
    "    \n",
    "    # Create gamma and run arc_since for image2\n",
    "    gamma2 = image2.copy()\n",
    "    gamma2 = gamma2 / S_param2\n",
    "    image2 = arc.arc_sinc(gamma2, C_param2)\n",
    "    image2[isnan(gamma2)] = nan\n",
    "    \n",
    "    # Partition image into subsections for noise suppression (multi-step process)\n",
    "    # Create M and N which are the number of subsections in each direction; fix() rounds towards zero\n",
    "    # NX and NY are the subsection dimensions\n",
    "    NX = N_pairwise\n",
    "    NY = N_pairwise\n",
    "    M = int(fix(lines / NY))\n",
    "    N = int(fix(samples / NX))\n",
    "    \n",
    "    # Create JM and JN, which is the remainder after dividing into subsections\n",
    "    JM = lines % NY\n",
    "    JN = samples % NX\n",
    "    \n",
    "    # Select the portions of images that are within the subsections\n",
    "    image1 = image1[0:lines - JM][:, 0:samples - JN]\n",
    "    image2 = image2[0:lines - JM][:, 0:samples - JN]\n",
    "    \n",
    "    # Split each image into subsections and run mean_wo_nan on each subsection\n",
    "    \n",
    "    # Declare new arrays to hold the subsection averages\n",
    "    image1_means = zeros((M, N))\n",
    "    image2_means = zeros((M, N))\n",
    "    \n",
    "    # Processing image1\n",
    "    # Split image into subsections with NY number of rows in each\n",
    "    image1_rows = split(image1, M, 0)\n",
    "    for i in range(M):\n",
    "        # split each section into subsections with NX number of columns in each\n",
    "        row_array = split(image1_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN andsave the value in another array\n",
    "        for j in range(N):\n",
    "            image1_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "    \n",
    "    # Processing image2\n",
    "    # Split image into sections with NY number of rows in each\n",
    "    image2_rows = split(image2, M, 0)\n",
    "    for i in range(M):\n",
    "        # split each section into subsections with NX number of columns in each\n",
    "        row_array = split(image2_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN an dsave the values in another array\n",
    "        for j in range(N):\n",
    "            image2_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "    \n",
    "    # Make an array for each image of where mean > 0 for both images\n",
    "    IND1 = logical_and((image1_means > 0), (image2_means > 0))\n",
    "    I1m_trunc = image1_means[IND1, ...]\n",
    "    I2m_trunc = iamge2_means[IND1, ...]\n",
    "    \n",
    "    R = corrcoef(I1m_trunc, I2m_trunc)\n",
    "    R = R[0,1]\n",
    "    RMSE = sqrt(sum((I1m_trunc - I2m_trunc)**2)/I1m_trunc.size)\n",
    "    \n",
    "    # Export the pair of heights for future scatter plot\n",
    "    filename = file_str + \"_I1andI2.json\"\n",
    "    R_RMSE_file = open(os.path.join(directory, \"output\", filename), 'w')\n",
    "    json.dump([I1m_trunc.tolist(), I2m_trunc.tolist()], R_RMSE_file)\n",
    "    R_RMSE_file.close()\n",
    "    \n",
    "    return R, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_error_metric_self(deltaS2, deltaC2, directory, N_self):\n",
    "    # load and read data from .mat file\n",
    "    # Samples and lines are calculated from the shape of the images\n",
    "    selffile_data = sio.loadmat(os.path.join(directory, \"output\", \"self.mat\"))\n",
    "    image1 = selffile_data['I1']\n",
    "    image2 = selffile_data['I2']\n",
    "    lines = int(image1.shape[0])\n",
    "    samples = int(image1.shape[1])\n",
    "    \n",
    "    # set the S and C parameters to the average S and C plus the delta values\n",
    "    S_param2 = 0.65 + deltaS2\n",
    "    C_param2 = 13 + deltaC2\n",
    "    \n",
    "    # Create gamma and run arc_since for image2\n",
    "    gamma2 = image2.copy()\n",
    "    gamma2 = gamm2 / S_param2\n",
    "    image2 = arc.arc_sinc(gamma2, C_param2)\n",
    "    image2[isnan(gamma2)] = nan\n",
    "    \n",
    "    # Partition image into subsections for noise suppression (multi-step process)\n",
    "    # Create M and N which are the number of subsections in each direction\n",
    "    # NX and NY aer the subsection dimensions \n",
    "    NX = N_self\n",
    "    NY = N_eslf\n",
    "    M = int(fix(lines / NY))\n",
    "    N = int(fix(samples / NX))\n",
    "    \n",
    "    # Create JM and JN, which is the remainder after dividing into subsections\n",
    "    JM = lines % NY\n",
    "    JN = samples % NX\n",
    "    \n",
    "    # Select the portions of images that are within the subsections\n",
    "    image1 = image1[0:lines - JM][:, 0:samples - JN]\n",
    "    image2 = image2[0:lines - JM][:, 0:samlpes - JN]\n",
    "    \n",
    "    # Split each image into subsections and run mean_wo_nan on each subsection\n",
    "    \n",
    "    # Declare new array to hold subsection averages\n",
    "    image1_means = zeros((M, N))\n",
    "    image2_means = zeros((M, N))\n",
    "    \n",
    "    # Processing image1\n",
    "    # Split image into sections with NY number of rows in each\n",
    "    image1_rows = split(image1, M, 0)\n",
    "    for i in range(M):\n",
    "        # split each section subsections with NX number of columns in each \n",
    "        row_array = split(iamge_rows[i], N, 1)\n",
    "        # for each subsection shape take the mean without NaN and save the value in another array\n",
    "        for j in range(N):\n",
    "            image1_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "            \n",
    "    # Processing image2\n",
    "    # Split image into sections with NY number of rows in each\n",
    "    image2_rows = split(image2_rows[i], N, 1)\n",
    "    for i in range(M):\n",
    "        # split each section into subsections with NX number o fcolumns in each\n",
    "        for j in range(N):\n",
    "            image2_means[i, j] = mwn.mean_wo_nan(row_array[j])\n",
    "    \n",
    "    # Make an array for each imaeg where mean > 0 for both images\n",
    "    IND1 = logical_and((image1_means > 0), (image2_means > 0))\n",
    "    I1m_trunc = image1_means[IND1, ...]\n",
    "    I2m_trunc = image2_means[IND1, ...]\n",
    "    \n",
    "    R = corrcoef(I1m_trunc, I2m_trunc)\n",
    "    R = R[0, 1]\n",
    "    RMSE = sqrt(sum((I1m_trunc-I2m_trunc)**2)/I1m_trunc.size)\n",
    "    \n",
    "    filename = \"self_I1andI2.json\"\n",
    "    R_RMSE_file = open(os.path.join(directory, \"output\", filename), 'w')\n",
    "    json.dump([I1m_trunc.tolist(), I2m_trunc.tolist()], R_RMSE_file)\n",
    "    R_RMSE_file.close()\n",
    "    \n",
    "    return R, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mosaic(directory, mosaicfile): \n",
    "    \n",
    "    print(time.strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    # Making list of files\n",
    "    listofffile = ''\n",
    "    iterdir = [f for f in os.listdir('.') if os.path.isdir(f) and f.startswith('f')]\n",
    "    \n",
    "    for num, val in enumerate(iterdir):\n",
    "        os.chdir(os.path.join(directory, val))\n",
    "        tiffile = [f for f in os.listdir('.') if f.endswith('fsh.tif')]\n",
    "        abspth = str(pathlib.Path(tiffile).absolute())\n",
    "        listoffiles = listoffiles + ' ' + abspth\n",
    "    \n",
    "    os.chdir(directory)\n",
    "    print(directory)\n",
    "    subprocess.getoutput('gdalbuildvrt -seperate -srcnodata 255 -overwrite ' + os.path.join(directory, 'mosaic.vrt') + listoffiles)\n",
    "    subprocess.getoutput('gdal_translate -of GTiff -a_nodata 255 ' + os.path.join(directory, 'mosaic.vrt') + ' ' + os.path.join(dirextory, 'mosaic.tif'))\n",
    "    \n",
    "    # Load mosaic.tif and associated parameters - .tif\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    drier.Register()\n",
    "    img = gdal.Open(os.path.join(directory, 'mosaic.tif'))\n",
    "    ref_data = array(img.ReadAsArray())\n",
    "    refgeotrans = img.GetGeoTransform()\n",
    "    corner_lon = refgeotrans[0]\n",
    "    post_lon = refgeotrans[1]\n",
    "    corner_lat = refgeotrans[3]\n",
    "    post_lat = refgeotrans[5]\n",
    "    geo_width = img.RasterXSize\n",
    "    geo_lines = img.RasterYSize\n",
    "    \n",
    "    ######### average all of the overlappingpixels at the same area\n",
    "    ref_data = single(ref_data)\n",
    "    ref_data[ref_data==255] = NaN\n",
    "    avg = nanmean(ref_data, axis = 0)\n",
    "    avg[isnan(avg)] = 255\n",
    "    \n",
    "    ######### Create the final GeoTiff\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    \n",
    "    outRaster = driver.Create(os.path.join(directory, mosaicfile), geo_width, geo_lines)\n",
    "    outRaster.SetGeoTransform([corner_lon, post_lon, 0, corner_lat, 0, post_lat])\n",
    "    outband = outRaster.GetRasterBand(1)\n",
    "    \n",
    "    outband.WriteArray(avg)\n",
    "    outRasterSRS = osr.SpatialReference()\n",
    "    outRasterSRS.ImportFromEPSG(4326)\n",
    "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "    outband.FlushCache()\n",
    "    \n",
    "    print(time.strftime(\"%H:%M:%S\"))\n",
    "    print((\"Final mosaic generation done!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scatterplot_density(x, y, bin_size = 100, threshold = 0.5):\n",
    "    values, xedges, yedges = np.histogram2d(x, y, bin_size)\n",
    "    xbin_center = xedges[0:-1] + (xedges[1] - xedges[0])/2\n",
    "    ybin_center = yedges[0:-1] + (yedges[1] - yedges[0])/2\n",
    "    max_den = np.max(values)\n",
    "    threshold_den = max_den * threshold\n",
    "    [BCX, BCY] = np.meshgrid(xbin_center, ybin_center)\n",
    "    values = values.transpose()\n",
    "    IND_den = (values >= threshold_den)\n",
    "    Hm_den = BCX[IND_den]\n",
    "    Pm_den = BCY[IND_den]\n",
    "    return Hm_den, Pm_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_scene_file(flagfilename, flag, directory):\n",
    "    \n",
    "    # Open the file\n",
    "    flagfile = open(os.path.join(directory, flagfilename))\n",
    "    \n",
    "    # Set default value for scene_file\n",
    "    data_array = [\"\", \"\", \"\", \"\", \"\"]\n",
    "    \n",
    "    # For each line in the file compare the line flag with the input flag\n",
    "    for line in flagfile:\n",
    "        \n",
    "        # Set the line values\n",
    "        line = line.strip().split()\n",
    "        lineflag = line[0]\n",
    "        \n",
    "        # Compare line and input flags\n",
    "        if int(lineflag) == flag:\n",
    "            data_array = list(line)\n",
    "            \n",
    "        # Close file\n",
    "        flagfile.close()\n",
    "        \n",
    "        # Print error message if input flag is not found\n",
    "        if(data_array[0] == \"\"):\n",
    "            print(\"EEROR: Invalid flag number for the given text file\")\n",
    "        \n",
    "        # Return Scene_file\n",
    "        return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_stand_height(scenes, edges, start_scene, iterations, linkfilename, flagfile, ref_file, maskfile, file_directory, filetypes = ['gif', 'json', 'kml', 'tif'], Nd_pairwise = 20, Nd_self = 20, N_pairwise = 20, N_self = 20, bin_size = 100, flag_sparse = 0,  flag_diff = 0, flag_error = 0, numLooks = 20, noiselevel = 0.0, flag_grad = 0, lat_shift = 0,lon_shift = 0):\n",
    "    \n",
    "    print(time.strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    # Set error warnings to ignore \"invalid value\" varnings caused by NaN values\n",
    "    seterr(invalid = 'ignore')\n",
    "    \n",
    "    if flag_sparse == 1:\n",
    "        Nd_self = 1\n",
    "\n",
    "    \n",
    "    if not os.path.exists(os.path.join(file_directory, 'output')):\n",
    "        os.mkdir(os.path.join(file_directory, 'output'))\n",
    "        \n",
    "    # Extract the correlation map, ks, and corner coordinated for each scene\n",
    "    athm.auto_tree_height_many(scenes, flagfile, file_directory, numLooks, noiselevel, flag_proc, flag_grad, lat_shift, lon_shift)\n",
    "    \n",
    "    if linkfilename == '-':\n",
    "        # Run intermediate_self() (Central scene and LiDAR overlap)\n",
    "        ins.intermediate_self(start_scene, flagfile, ref_file, maskfile, file_directory)\n",
    "        edge_array = array([])\n",
    "        print(time.strftime(\"%H:%M:%S\"))\n",
    "    else:\n",
    "        # Read in the list of edges\n",
    "        edge_array = rlnk.read_linkfule(edges, linkfilename, file_directory)\n",
    "        # Calculate the overlap areas between the different scenes and the LiDAR (or other groundtruth)\n",
    "        inter.intermediate(edges, start_scene, edge_array, maskfile, flagfile, ref_file, file_directory)\n",
    "        \n",
    "    # Mosaic th einterferograms\n",
    "    amn.auto_mosaicking_new(scenes, edges, start_scene, iterations, edge_array, flag_directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "    \n",
    "    # Store the delta S and C values for each scene\n",
    "    wSC.write_deltaSC(scenes, iterations, flagfile, file_directory)\n",
    "    \n",
    "    # Create the tree height map\n",
    "    wmn.writefile_new(scenes, flagfile, maskfile, file_directory, filetypes)\n",
    "    \n",
    "    if flag_diff == 1:\n",
    "        # Create the diff_height map\n",
    "        wdm.write_diff_height_map(start_scene, ref_file, flagfile, maskfile, file_directory, filetypes)\n",
    "    \n",
    "    \n",
    "    # Run cal_error_metric() when error metrics/scatter plots are needed\n",
    "    if flag_error == 1:\n",
    "        # Load the dp vector from the final iteration\n",
    "        filename = \"SC_%d_iter.json\" % iterations\n",
    "        \n",
    "        iter_file = open(os.path.join(file_directory, \"output\", filename))\n",
    "        file_data = json.load(iter_file)\n",
    "        iter_file.close()\n",
    "        dp = array(file_data[0])\n",
    "        # Run cal_error_metric() and create a json containing all of the \"pairwise\" and \"self\" R & RMSE errror measures\n",
    "        Y = cem.cal_error_metric(dp. edges, start_scene, edge_array, file_directory, N_pairwise, N_self)\n",
    "        output_file = open(os.path.joinn(file_directory, \"output\", \"error_metric.json\"), 'w')\n",
    "        json.dump([Y.tolist()], output.file)\n",
    "        output_file.close()\n",
    "        print(\"cal_error_metric file written at \" + (time.strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate(edges, start_scene, linkarray, maskfile, flagfile, ref_file, directory):\n",
    "    # For each edge run intermediate_pairwise\n",
    "    for i in range(edges):\n",
    "        inp.intermediate_pairwise(linkarray[i, 0], linkarray[i, 1], flagfile, maskfile, directory)\n",
    "        print((\"%d edge file(s) created at \" % (i + 1)) + (time.strftime(\"%H:%M:%S\")))\n",
    "        \n",
    "    # Run intermediate_self() (Central scene and LiDAR overlap)\n",
    "    ins.intermediate_self(start_scene, flagfile, ref_file, maskfile, directory)\n",
    "    \n",
    "    print(\"intermediate() complete - overlap areas calculated at \" + (time.strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_pairwise(flag1, flag2, flagfile, maskfile, directory):\n",
    "    \n",
    "    # Get flag-scene file data\n",
    "    scene1_data = fsf.flag_scene_file(flagfile, flag1, directory)\n",
    "    scene2_data = fsf.flag_scene_file(flagfile, flag2, directory)\n",
    "    \n",
    "    # Set file names based on flags\n",
    "    filename1 = scene1_data[1]\n",
    "    filename2 = scene2_data[1]\n",
    "    \n",
    "    # Set the image folder names \n",
    "    image1_folder = \"f\" + scene1_data[4] + \"_o\" + scene1_data[5]\n",
    "    image2_folder = \"f\" + scene2_data[4] + \"_o\" + scene2_data[5]\n",
    "    \n",
    "    innf1 = os,path.join(directory, image1_folder, filename1 + \"_orig.mat\")\n",
    "    file1 = sio.loadmat(innf1)\n",
    "    corr1 = file1['corr_vs']\n",
    "    kz1 = file1['ks'][0][0]\n",
    "    coords1 = file1['coords'][0]\n",
    "    \n",
    "    #  Load second image file and associated parameters\n",
    "    \n",
    "    innf2 = os.path.join(directory + image2_folder + filename2 + \"_orig.mat\")\n",
    "    file2 = sio.loadmat(innf2)\n",
    "    corr2 = file2['corr_vs']\n",
    "    kz2 = file2['k2'][0][0]\n",
    "    coords2 = file2['coords'][0]\n",
    "    \n",
    "    # Set D constant ----- D = 1 arc second\n",
    "    D = 2.7777778 * (10**-4)\n",
    "    \n",
    "    # Remove non-forest from both images\n",
    "    if maskfile != '-':\n",
    "        corr1 = rnf.remove_nonforest(corr1, coords1, maskfile, directory)\n",
    "        coor2 = rnf.remove_nonforest(corr2, coords2, maslfile, directory)\n",
    "    \n",
    "    # Set the image boundaries\n",
    "    north1 = coords1[0]\n",
    "    south1 = coords1[1]\n",
    "    west1 = coords1[2]\n",
    "    east1 = coords1[3]\n",
    "    north2 = coords2[0]\n",
    "    south2 = coords2[1]\n",
    "    west2 = coords2[2]\n",
    "    east2 = coords[3]\n",
    "    \n",
    "    # Determine boudaries of overlap area\n",
    "    overlap_north = min(north1, north2)\n",
    "    overlap_south = max(south1, south2)\n",
    "    overlap_east = min(east1, east2)\n",
    "    overlap_west = max(west1, west2)\n",
    "    \n",
    "    # Calculate overlap boudaries in coordinates of each image (ex image1[100-1200] vs image2[0-200])\n",
    "    xw1 = int(round(((overlap_west - west1) / D) + 1))\n",
    "    xe1 = int(round(((overlap_east - east1) / D) + 1))\n",
    "    xn1 = int(round((-(overlap_north - north1) / D) + 1))\n",
    "    xs1 = int(round((-(overlap_south - south1) / D) + 1))\n",
    "    xw2 = int(round(((overlap_west - west2) / D) + 1))\n",
    "    xe2 = int(round(((overlap_east - east2) / D) + 1))\n",
    "    xn2 = int(round((-(overlap_north - north2) / D) + 1))\n",
    "    xs2 = int(round((-(overlap_south - south2) / D) + 1))\n",
    "    \n",
    "    # Set overlap sections from each image\n",
    "    I1 = corr1[xw1 - 1:xe1][:, xn1 - 1:xs1]\n",
    "    I2 = corr2[xw2 - 1:xe2][:, xn2 - 1:xs2]\n",
    "    \n",
    "    # Set average S and C parameters based on the average S and C (0 < s < 1, 0 < c < 20 so s = 0.65 and c = 13)\n",
    "    S_param1 = 0.65\n",
    "    C_param1 = 13\n",
    "    S_param2 = 0.65\n",
    "    C_param2 = 13\n",
    "    \n",
    "    # Create grid for image1 \n",
    "    [Dy1, Dx1] = I1.shape\n",
    "    x1 = linspace(0, 1, Dx1)\n",
    "    y1 = linspace(0, 1, Dy1)\n",
    "    [X1, Y1] = meshgrid(x1, y1)\n",
    "    \n",
    "    # Create grid for image2\n",
    "    [Dy2, Dx2] = I2.shape\n",
    "    x2 = linspace(0, 1, Dx2)\n",
    "    y2 = linspace(0, 1, Dy2)\n",
    "    [X2, Y2] = meshgrid(x2, y2)\n",
    "    \n",
    "    # Set NaN values to -100 to avoid interpolation errors\n",
    "    I1[isnan(I1)] - 100\n",
    "    I2[isnan(I2)] - 100\n",
    "    \n",
    "    # Co-register the two images\n",
    "    I2 = griddate((X2.flatter(), Y2.flatten()), I2.flatten(), (X1, Y1), method = 'nearest')\n",
    "    \n",
    "    # Reset NaN values\n",
    "    IND1 = (I1 == -100)\n",
    "    IND2 = (I2 == -100)\n",
    "    IND = logical_or(IND1, IND2)\n",
    "    I1[IND] = NaN\n",
    "    I2[IND] = NaN\n",
    "    \n",
    "    # Save link file using MAT\n",
    "    linkfilename = \"%s_%s.mat\" % (int(flag1), int(flag2))\n",
    "    linkfile = os.path.join(directory, \"output\", linkfilename)\n",
    "    sio.savemat(linkfile, {'I1':I1, 'I2':I2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_self(start_scene, flagfile, ref_file, maskfile, directory):\n",
    "    \n",
    "    # Set Scene data, file name, and image folder name\n",
    "    scene2_data = fsf.flag_scene_file(flagfile, start_scene, directory)\n",
    "    filename2 = scene2_data[1]\n",
    "    image_folder = \"f\" + scene2_data[4] + \"_o\" + scene2_data[5]\n",
    "    \n",
    "    # Set D constant --- D = 1 arc second, this parameter is based on the use of ALOS data\n",
    "    D = 2.77777778 * (10**-4) # fpr tracy test case\n",
    "    \n",
    "    # Load central image file and associated parameters\n",
    "    inf2 = os.path.join(directory, image_folder, filename2 + \"_orig.mat\")\n",
    "    file2 = sio.loadmat(inf2)\n",
    "    corr2 = file2['corr_vs']\n",
    "    kz2 = file2['kz'][0][0]\n",
    "    coords2 = file2['coords'][0]\n",
    "    \n",
    "    # Remove non-forest from the image\n",
    "    if maskfile != '-':\n",
    "        corr2 = rnf.remove_nonforest(coor2, coords2, maskfile, directory)\n",
    "    \n",
    "    # Load LiDAR files and associated parameters - .tif\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    driver.Register()\n",
    "    img = gdal.Open(os.path.join(directory, ref_file))\n",
    "    ref_data = array(img.ReadAsArray())\n",
    "    refgeotrans = img.GetGeoTransform()\n",
    "    corner_lon = refgeotrans[0]\n",
    "    post_lon = refgeotrans[1]\n",
    "    corner_lat = refgeotrans[3]\n",
    "    post_lat = refgeotrans[5]\n",
    "    width = img.RasterXSize\n",
    "    lines = img.RasterYSize\n",
    "    \n",
    "    # Set LiDAR parameters into correct format\n",
    "    corr1 = ref_data.transpose()\n",
    "    corr1[corr1 < 0] = NaN # Set margin areas to NaN\n",
    "    coords1 = array([corner_lat, corner_lat + (lines * post_lat), corner_lon, corner_lon + (width * post_lon)])\n",
    "    \n",
    "    # Set the image boundaries\n",
    "    north1 = coords1[0]\n",
    "    south1 = coords1[1]\n",
    "    west1 = coords1[2]\n",
    "    east1 = coords1[3]\n",
    "    north2 = coords2[0]\n",
    "    south2 = coords2[1]\n",
    "    west2 = coords2[2]\n",
    "    east2 = coords2[3]\n",
    "    \n",
    "    # Determine boundaries of the overlap area\n",
    "    overlap_north = min(north1, north2)\n",
    "    overlap_south = max(south1, south2)\n",
    "    overlap_east = min(east1, east2)\n",
    "    overlap_west = max(west1, west2)\n",
    "    \n",
    "    # calculate overlap boundaries in coordinates of image2 (ex image1[1000-1200], image2[0-200])\n",
    "    xw2 = int(round(((overlap_west - west2) / D) + 1))\n",
    "    xe2 = int(round(((overlap_east - east2) / D) + 1))\n",
    "    xn2 = int(round((-(overlap_north - north2) / D) + 1))\n",
    "    xs2 = int(round((-(overlap_south - south2) / D) + 1))\n",
    "    \n",
    "    # Set overlap sections for the LiDAR and SAR images\n",
    "    I1 = corr1.copy()\n",
    "    I2 = corr2[xw2 - 1:xe2][:, xn2 - 1:xs2]\n",
    "    \n",
    "    # Set average S and C parameters based on the average S and C (0 < s < 1, 0 < c < 20 so S = 0.65 and C = 13)\n",
    "    S_param2 = 0.65\n",
    "    C_param2 = 13\n",
    "    \n",
    "    # Create grid for image2\n",
    "    [Dy2, Dx2] = I2.shape\n",
    "    x2 = linspace(0, 1, Dx2)\n",
    "    y2 = linspace(0, 1, Dy2)\n",
    "    [X2, Y2] = meshgrid(x2, y2)\n",
    "    \n",
    "    # Set NaN values to -100 to avoid interpolation erros\n",
    "    I1[isnan(I1)] = -100\n",
    "    I2[isnan(I2)] = -100\n",
    "    \n",
    "    # Co-register the two images\n",
    "    I2 = griddata((X2.flatten(), Y2.flatten()), I2.flatten(), (X1, Y1), method = 'nearest')\n",
    "    \n",
    "    # Reset NaN values\n",
    "    IND1 = (I1 == -100)\n",
    "    IND2 = (I2 == -100)\n",
    "    IND = logical_or(IND1, IND2)\n",
    "    I1[IND] = NaN\n",
    "    I2[IND] = NaN\n",
    "    \n",
    "    # Save Link file using JSON\n",
    "    linfilename = \"self.mat\"\n",
    "    linkfile = os.path.join(directory, \"output\", linkfilename)\n",
    "    sio.savemat(linkfile, {'I1':I1, 'I2':I2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_deltaSC(dp, edges, scenes, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse):\n",
    "    \n",
    "    # run cal_KB\n",
    "    y = cKB.cal_KB(dp, edges, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "    \n",
    "    # Create a blank array for the Jacobi matrix\n",
    "    jacobi = zeros(4 * scenes * (edges + 1)) \n",
    "    jacobi = reshape(jacobi, (2 * (edges + 1), scenes * 2))\n",
    "    \n",
    "    \n",
    "    # Fill in the jacobi matrix\n",
    "    for i in range(scenes): \n",
    "        # fill K section\n",
    "        temp = dp.copy()\n",
    "        temp[2 * i] = temp[2 * i] + 0.1\n",
    "        temp = cKB.cal_KB(temp, edges, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "        jacobi[:, 2 * i] = reshape(((temp - y) / 0.1), (2 * (edges + 1), ))\n",
    "        \n",
    "        # fill b section\n",
    "        temp = dp.copy()\n",
    "        temp[(2 * i) + 1] = temp[(2 * i) + 1] + 1\n",
    "        temp = cKB.cal_KB(temp, edges, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "        jacobi[:, (2 * i) + 1] = reshape(((temp - y) / 1), (2 * (edges + 1), ))\n",
    "        \n",
    "    # Create matrix of target K and B values (K = 1, B = 0 in order K-B-K-B-K-B...)\n",
    "    target = zeros((edges + 1) * 2)\n",
    "    target[::2] = 1\n",
    "    \n",
    "    # Calculate the change in S and C\n",
    "    changeSC = dot(dot(linalg.inv(dot(jacobi.conj().transpose(), jacobi)), jacobi.conj().transpose()), (target - y))\n",
    "    \n",
    "    changeSC = changeSC + dp\n",
    "    YY = cKB.cal_KB(changeSC, edges, start_scene, linkarray, directory, Nd_pairwise, Nd_self, bin_size, flag_sparse)\n",
    "    res = sum((YY - target) ** 2)\n",
    "    \n",
    "    # Return changeSC and res\n",
    "    return changeSC, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_wo_nan(A):\n",
    "    \n",
    "    # copy and flatten A\n",
    "    B = A.copy().flatten(1)\n",
    "    \n",
    "    # Remove NaN values from B\n",
    "    B = B[~isnan(B)]\n",
    "    \n",
    "    # Return the mean of B\n",
    "    return mean(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_geo_data(coord_file, directory):\n",
    "    \n",
    "    # Set filename for file to be searched\n",
    "    filename = directory + coord_file\n",
    "    \n",
    "    # Read parameters based on file type Ge0TIFF or ROI_PAC text file)\n",
    "    if(coord_file[-3:] == \"tif\"):\n",
    "        # Read GeoTIFF\n",
    "        driver = gdal.GetDriverByName('GTIFF')\n",
    "        driver.Register()\n",
    "        image = gdal.Open(filename)\n",
    "        refgeotrans = image.GetGeoTransform()\n",
    "        corner_long = refgeotrans[0]\n",
    "        post_long = refgeotrans[1]\n",
    "        corner_lat = refgeotrans[3]\n",
    "        post_lat = refgeotrans[5]\n",
    "        width = image.RasterXSize\n",
    "        nlines = image.RasterYSize\n",
    "        \n",
    "    else:\n",
    "        # Read ROI_PAC text file\n",
    "        for line in open(filename):\n",
    "            if line.startswith(\"width\"):\n",
    "                width = int(line.strip().split()[1])\n",
    "            elif line.startwith(\"nlines\"):\n",
    "                nlines = int(line.strip().split()[1])\n",
    "            elif line.startswith(\"corner_lat\"):\n",
    "                corner_lat = float(line.strip().split()[1])\n",
    "            elif line.startswith(\"corner_lon\"):\n",
    "                corner_long = float(line.strip().split()[1])\n",
    "            elif line.startswith(\"post_lat\"):\n",
    "                post_lat = float(line.strip().split()[1])\n",
    "            elif line.startswith(\"post_lon\"):\n",
    "                post_lon = float(line.strip().split()[1])\n",
    "        \n",
    "        return width, nlines, corner_lat, corner_long, post_lat, post_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_link(edges, filename, directory):\n",
    "    if edges > 0:\n",
    "        # open the file\n",
    "        linkfile = open(directory + filename)\n",
    "        \n",
    "        # Create output array\n",
    "        linkarray = zeros(edges *2).reshape(edges, 2)\n",
    "        \n",
    "        # Set line counter\n",
    "        counter = 0\n",
    "        \n",
    "        # for each line in the file compare the line flag with the input flag\n",
    "        for line in linkfile:\n",
    "            \n",
    "            # Set array values from each line\n",
    "            line = line.strip().split()\n",
    "            linkarray[counter][0] = line[0]\n",
    "            linkarray[counter][1] = line[1]\n",
    "            \n",
    "            # Increment counter\n",
    "            counter += 1\n",
    "        \n",
    "        # Close file\n",
    "        linkfile.close()\n",
    "        \n",
    "        # Return linkarray\n",
    "        return linkarray\n",
    "    else:\n",
    "        linkarray = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_corr_bias(C, L):\n",
    "    \n",
    "    # Set m and D arrays the correspond to ROI_PAC.\n",
    "    k = 1.0\n",
    "    D = double(arange(0, 1, 0.01))\n",
    "    m = array([])\n",
    "    for i in range(D.shape[0]):\n",
    "        m = append(m, gamma(L)* gamma(1 + k/2) / gamma(L + k/2) * hyper([1 + k/2, L, L], [L + k/2, 1], D[i]**2)*(1-D[i]**2)**L)\n",
    "    m = double(m)\n",
    "    p = arange(0, mine(m), 0.01)\n",
    "    p = double(p)\n",
    "    m = append(append(p, m), array([1]))\n",
    "    D = append(append(0*p, D), array([1]))\n",
    "    \n",
    "    # Run interpolation\n",
    "    set_interp = interp1d(m, D, kind = 'cubic')\n",
    "    YC = set_interp(C)\n",
    "    \n",
    "    return YC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonforest(I, func_coords, maskfile, directory):\n",
    "    \n",
    "    # Load mask file as a GeoTIFF\n",
    "    maskfile = gdal.Open(os.path.join(directory, maskfile))\n",
    "    mask = array(maskfile.ReadAsArray())\n",
    "    \n",
    "    # Set any NaN values to 1 (aka not a forest)\n",
    "    mask[isnan(mask)] = 1\n",
    "    \n",
    "    # Get mask geo parameters\n",
    "    width = maskfile.RasterXSize\n",
    "    nlines = maskfile.RasterYSize\n",
    "    maskgeotrans = maskfile.GetGeoTransform()\n",
    "    corner_lon = maskgeotrans[0]\n",
    "    post_lon = maskgeotrans[1]\n",
    "    corner_lat = maskgeotrans[3]\n",
    "    post_lat = maskgeotrans[5]\n",
    "    \n",
    "    # Transpose mask so that it matches orientation of the radar data\n",
    "    mask = mask.transpose()\n",
    "    widthT = nlines\n",
    "    nlinesT = width\n",
    "    \n",
    "    # Set coordinates based on file parameters\n",
    "    file_coords = array([corner_lat, (corner_lat + (nlinesT - 1.0) * post_lat), corner_lon, (corner_lon + (widthT - 1.0) * post_lon)])\n",
    "    \n",
    "    # Calculate overlap boundaries in new coordinate system\n",
    "    xw = int(round(((func_coords[2] - file_coords[2]) / post_lon) + 1))\n",
    "    xe = int(round(((func_coords[3] - file_coords[2]) / post_lon) + 1))\n",
    "    xn = int(round(((func_coords[0] - file_coords[0]) / post_lat) + 1))\n",
    "    xs = int(round(((func_coords[1] - file_coords[0]) / post_lat) + 1))\n",
    "    \n",
    "    # trim mask\n",
    "    mask = logical_not(mask[xw-1:xe][:, xn-1:xs])\n",
    "    \n",
    "    # Get size of image and mask\n",
    "    [m, n] = I.shape\n",
    "    [M, N] = mask.shape\n",
    "    \n",
    "    # Make range of values from 0 - 1 based on M and N (not including 1), and run linspace\n",
    "    x = linspace(0, 1, N, endpoint = False)\n",
    "    y = linspace(0, 1, M, endpoint = False)\n",
    "    [X, Y] = meshgrid(x, y)\n",
    "    \n",
    "    # make range of values from 0 - 1 based on m and n (not including 1), and run linspace\n",
    "    xp = linspace(o, 1, n, endpoint = False)\n",
    "    yp = linspace(0, 1, m, endpoint = False)\n",
    "    [XY, YP] = meshgrid(xp, yp)\n",
    "    \n",
    "    # Run interpolation\n",
    "    O = sciint.griddate((X.flatten(), Y.flatten()), mask.flatten(), (XP, YP), method = 'nearest')\n",
    "    O = double(O)\n",
    "    O[O == 0] = NaN\n",
    "    O = I * O\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(x, y, win_size = 0.5, threshold = 5):\n",
    "    \n",
    "    # Initialize other variables\n",
    "    outliers_ind = []\n",
    "    ind_x = zeros(x.size)\n",
    "    ind_y = zerps(x.size)\n",
    "    ind = zeros(x.size)\n",
    "    \n",
    "    # For each value in x check more or less neighboring points within givenwind than the given threshold\n",
    "    for i in range(x.size):\n",
    "        # set base equal to a pair of x, y values\n",
    "        current_x = x[i]\n",
    "        current_y = y[i]\n",
    "        \n",
    "        # for each x and y check if they are within +- the window from the current x(i) and y(i)\n",
    "        # store a list of where both a and y are within the window in the array\n",
    "        ind_x = (x > current_x - win_size) & (x < current_x + win_size)\n",
    "        ind_y = (y > current_y - win_size) & (y < current_y + win_size)\n",
    "        ind = ind_x & ind_y\n",
    "        \n",
    "        # if for the current i ther are fewer nearby point (within window) than the threshold\n",
    "        if sum(ind) <= threshold:\n",
    "            # then append it to the outliers array\n",
    "            outliers_ind.append(i)\n",
    "            \n",
    "    # Make new copies of x and y and delete all of the outlying points\n",
    "    XX = delete(x, outliers_ind)\n",
    "    YY = delete(y, outliers_ind)\n",
    "    \n",
    "    # Return XX and YY\n",
    "    return XX, YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_deltaSC(scenes, N, flagfile, directory):\n",
    "    # Load dp from final iteration .json file\n",
    "    filename = \"SC_%d_iter.json\" % N\n",
    "    selffile = open(os.path.join(directory, \"output\", filename))\n",
    "    selffile_data = json.load(selffile)\n",
    "    dp = array(selffile_data[0])\n",
    "    \n",
    "    # For each scene name the file, create delta S and C, and save them to the file\n",
    "    for i in rance(scenes):\n",
    "        \n",
    "        # set file name\n",
    "        scene_data = fsf.flag_scene_file(flagfile, i + 1, directory)\n",
    "        filename = scene_data[1]\n",
    "        image_folder = \"f\" + scene_data[4] + \"_o\" + scene_data[5] + \"/\"\n",
    "        \n",
    "        # Calculate delta S and C\n",
    "        DS = dp[2 * i]\n",
    "        DC = dp[(2 * i) + 1]\n",
    "        \n",
    "        # Save DS and DC to output .json file\n",
    "        outfile = open(os.path.join(directory, image_folder, filename + '_tempD.json'),\"w\")\n",
    "        json.dump([DS, DC], outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "    print(\"write_deltaSC completed at \" + (time.strftime(\"%H:%M:%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_diff_height_map(start_scene, ref_file, flagfile, maskfile, directory, output_files):\n",
    "    \n",
    "    if isinstance(start_scene, int) == 1:\n",
    "        # Load and read data from .mat file\n",
    "        # Samples and lines are calculated from the shape of the images\n",
    "        file_data = sio.loadmat(os.path.join(directory, \"output\", \"self.mat\"))\n",
    "        lidar = file_data['I1']\n",
    "        corr_vs = file_data['I2']\n",
    "        \n",
    "        \n",
    "        # Load and read data from temp .json files\n",
    "        scene_data = fsf.flag_scene_file(flagfile, start_scene, directory)\n",
    "        filename = scene_data[1]\n",
    "        image_folder = \"f\" + scene_data[4] + \"_o\" + scene_data[5] + \"/\"\n",
    "        file_tempD = open(os.path.join(directory, image_folder, filename + \"_tempD.json\"))\n",
    "        B = json.load(file_tempD)\n",
    "        \n",
    "        \n",
    "        # Set S and C parameters based on the default and data from B\n",
    "        S_param = 0.65 + B[0]\n",
    "        C_param = 13 + B[1]\n",
    "        \n",
    "        # Run sinc model to calculate the heights\n",
    "        gamma = corr_vs.copy()\n",
    "        gamma = gamma / S_param\n",
    "        height = arc.arc_sinc(gamma, C_param)\n",
    "        \n",
    "        # Calculate the diff_heigh map (diviation f the InSAR ivnersted height away from the lidar height)\n",
    "        diff_height = lidar - height\n",
    "        \n",
    "        # transpose height to correctly align it (ie so it isn't rotated inrelation to an underlying map)\n",
    "        diff_height = diff_height.transpose()\n",
    "        \n",
    "        # Get rif of NaN so future processing software doesn't error\n",
    "        diff_heigh[isnan(diff_height)] = 255\n",
    "        \n",
    "        # Write all the desired output file types for th eforest diff_height map\n",
    "        for filetype in output_files:\n",
    "            wft.write_file_type(diff_height, \"diff_height\", filename, os.path.joi(directory,image_folder), filetype, 0, ref_file)\n",
    "        \n",
    "        print(\"all diff_height output files written at \" + (time.strftime(\"%H:%M:%S\")))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file_type(data, outtype, filename, directory, filetype, coords, ref_file = \"\"):\n",
    "    # Use if/else to determine the desired type of file output\n",
    "    if (filename[-8:] == \"_255_255\"):\n",
    "        outfilename = filename[:-4]\n",
    "    elif((filename[-4:] == \"_fsh\") or (filename[-5:] == \"_diff\") or (filename[-4:] == \"_255\")):\n",
    "        outfilename = filename\n",
    "    else:\n",
    "        if(outtype == \"stand_height\"):\n",
    "            outfilename = filename = \"_fsh\"\n",
    "        elif(outtype == \"diff_height\"):\n",
    "            outfilename = filename + \"_diff\"\n",
    "    \n",
    "    # use if/esle to determine the desired type of file output\n",
    "    # Create .gif output\n",
    "    if(filetype == \"gif\"):\n",
    "        # Check if a 0-255 .tif with the same filename already exists, and if not create it.\n",
    "        if (os.path.isfile(os.path.join(directory, outfilename + \"_255.tif\")) == True):\n",
    "            gif_img = Image.open(os.path.join(directory, outfilename + \"_255.tif\"))\n",
    "        else:\n",
    "            # Set array in a 0-255 range for gif/kml\n",
    "            # Get dimensions of array and then flatten for use with nonzero()\n",
    "            (row, col) = data.shape\n",
    "            data = data.flatten()\n",
    "            # Get the nonzero indices and min/max\n",
    "            nz_IND = nonzero(data)\n",
    "            nz_min = data[nz_IND[0]].min()\n",
    "            mz_max = data[nz_IND[0]].max()\n",
    "            # Set the scaled values\n",
    "            data255 = data.copy()\n",
    "            data255[nz_IND[0]] = (data[nz_IND[0]] - nz_min) * (255 / (nz_max - nz_min)) + 1\n",
    "            # Reshape the array of scaled values\n",
    "            data255 = reshape(data255, (row, col))\n",
    "            data = reshape(data, (row, cal))\n",
    "            \n",
    "            # Write 0-255 .tif\n",
    "            write_file_type(data255, outtype, outfilename + \"_255\", directory, \"tif\", coords,ref_file)\n",
    "            gif_img.save(os.path.join(directory, outfilename + '.json'), 'w')\n",
    "            \n",
    "        # Create the .gif\n",
    "        gif_img.save(os.path.join(directory, outfilename + \"_255.gif\"), \"GIF\", transparency=0)\n",
    "    \n",
    "    # Create .json output\n",
    "    elif(filetype == \"json\"):\n",
    "        jsonfile = open(os.path.join(directory, outfilename + '.json'), 'w')\n",
    "        json.dump([data.tolist()], jsonfile)\n",
    "        jsonfile.close()\n",
    "            \n",
    "    # Create .kml output\n",
    "    elif(filetype[3] == \"kml\"):\n",
    "        # Determine the realname based on whether or not a single image is being processed or a pair\n",
    "        if(filename[3] == \"_\"): # pair\n",
    "            realname = filename[:31]\n",
    "        else:\n",
    "            realname = filename[:23]\n",
    "            \n",
    "        # Read ge location information in from a text or geotiff file depending on outtype\n",
    "        if(outtype == \"stand_height\"):\n",
    "            (width, lines, north, west, lat_step, long_step) = rgd.read_geo_data(realname + \"_geo.txt\", directory)\n",
    "            north = coords[0]\n",
    "            south = coords[1]\n",
    "            west = coords[2]\n",
    "            east = coords[3]\n",
    "        elif(outtype == \"diff_height\"):\n",
    "            (width, lines, north, west, lat_step, long_step) = rgd.read_geo_data(ref_file, directory[:-10])\n",
    "            north = coords[0]\n",
    "            south = coords[1]\n",
    "            west = coords[2]\n",
    "            east = coords[3]\n",
    "            \n",
    "        # Create the .kml\n",
    "        kml = simplekml.Kml()\n",
    "        arraykml = kml.newgroundoverlay(name = outfilename)\n",
    "        arraykml = kml.icon.href = os.path.join(directory, outfilename + \"_255.gif\")\n",
    "        arraykml.latlonbox.north = north\n",
    "        arraykml.latlonbox.south = south\n",
    "        arraykml.latlonbox.east = east\n",
    "        arraykml.latlonbox.west = west\n",
    "        kml.save(os.path.join(directory, outfilename + \"_255.kml\"))\n",
    "            \n",
    "    # Create .mat output\n",
    "    elif(filetype == \"mat\"):\n",
    "        sio.savemat(os.path.join(directory, outfilename + '.mat'), {'data':data})\n",
    "        \n",
    "        \n",
    "    # Create .tif output\n",
    "    elif(filetype == \"tif\"):\n",
    "        # determine the realname based on whether or not a single image is being processed or a pair\n",
    "        if(filename[3] == \"_\"):\n",
    "            realname = filename[:31]\n",
    "        else:\n",
    "            realname = filename[:23]\n",
    "            \n",
    "        # Read geo location information in from a text or geotiff file depening on outtype\n",
    "        if(outtype == \"stand_height\"):\n",
    "            (cols, rows, corner_lat, corner_long, lat_step, lomg_step) = rgd.read_geo_data(realname + \"_geo.txt\", directory)\n",
    "            corner_lat = coords[0]\n",
    "            corner_long = coords[2]\n",
    "            lat_step = -2.777777777778 * (10**-4)\n",
    "            long_step = 2.777777777778 * (10**-4)\n",
    "        elif(outtype == \"diff_height\"):\n",
    "            (cols, rows, corner_lat, corner_long, lat_step, long_step) = rgd.read_geo_data(ref_file, directory[:-10])\n",
    "            selffile_data = sio.loadmat(directory[:-10] + \"output/\" + \"self.mat\")\n",
    "            image1 = selffile_data[\"I1\"]\n",
    "            cols = int(image1.shape[0])\n",
    "            rows = int(image1.shape[1])\n",
    "            lat_step = -2.77777778 * (10**-4)\n",
    "            long_step = 2.77777778 * (10**-4)\n",
    "            \n",
    "        # Create the GeoTiff\n",
    "        driver = gdal.GetDriverByName('GTiff')\n",
    "        outRaster = driver.Create(os.path.join(directory, outfilename + \".tif\"), cols, rows)\n",
    "        outRaster.SetGeoTransform([corner_long, long_step, 0, corner_lat, 0, lat_step])\n",
    "        outband = outRaster.GetRasterBand(1)\n",
    "        outband.WriteArray(data)\n",
    "        outRasterSRS = osr.SpatialReference()\n",
    "        ourRasterSRS.ImportFromESPG(4326)\n",
    "        outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "        outband.FlushCache()\n",
    "        \n",
    "    else:\n",
    "        # Error message\n",
    "        print(\"Error: the selected file type is invalid. Please try again and choose a different outut format.\")\n",
    "        print(\"You selected %s\" % filetype)\n",
    "        print(\"File types available: .gif, .json, .kml, .mat, .tif --input without the., such as kml instead of .kml\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mapfile_new(scenes, flagfile, maskfile, directory, output_files):\n",
    "    \n",
    "    # For each scene\n",
    "    for i in range(scenes):\n",
    "        \n",
    "        # Set the filename\n",
    "        scene_data = fsf.flag_scene_file(flagfile, i + 1, directory)\n",
    "        filename = scene_data[1]\n",
    "        image_folder = \"f\" + scene_data[4] + \"_o\" + scene_data[5]\n",
    "        \n",
    "        # Load first image file and associated parameters\n",
    "        \n",
    "        file1 = sio.loadmat(os.path.join(directory, image_folder, filename + \"_orig.mat\"))\n",
    "        corr_vs = file1['corr_vs']\n",
    "        coords = file1['coords'][0]\n",
    "        \n",
    "        # Load and read data from temp .json files\n",
    "        file_tempD = open(os.path.join(directory, image_folder, filename + \"_tempD.json\"))\n",
    "        B = json.load(file_tempD)\n",
    "        \n",
    "        # Set S and C parameters based on the default and data from B\n",
    "        S_param = 0.65 + B[0]\n",
    "        C_param = 13 + B[1]\n",
    "        \n",
    "        # Run interpolation to calculate the heights\n",
    "        gamma = coor_vs.copy()\n",
    "        gamma = gamma / S_param\n",
    "        height = arc.arc_sinc(gamma, C_param)\n",
    "        height[isnan(gamma)] = nan\n",
    "        \n",
    "        # Mask out non-forest areas\n",
    "        if maskfile != '-':\n",
    "            forest_only_height = rnf.remove_nonforest(height, coords, maskfile, directory)\n",
    "        else:\n",
    "            forest_only_height = height\n",
    "        \n",
    "        # Transpose height to correctly align it (ie so it isn't rotated in relation to an underlying map)\n",
    "        forest_only_height = forest_only_height.transpose()\n",
    "        \n",
    "        # Get rid of NaN so future processing software doesn't error\n",
    "        forest_only_height[isnan(forest_only_height)] = 255\n",
    "        \n",
    "        # Write all the desired output file types for the forest height map\n",
    "        for filetype in output_files:\n",
    "            wft.wrtie_t\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
